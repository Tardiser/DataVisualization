Part
9          Business Intelligence
Chapter 31 Data Warehousing Concepts     1149
Chapter 32 Data Warehousing Design       1181
Chapter 33 OLAP                          1204
Chapter 34 Data Mining                   1232
                      www.it-ebooks.info


www.it-ebooks.info

     Chapter
31Chapter Objectives
                             Data Warehousing
                             Concepts
  In this chapter you will learn:
  n   How data warehousing evolved.
  n   The main concepts and benefits associated with data warehousing.
  n   How Online Transaction Processing (OLTP) systems differ from data warehousing.
  n   The problems associated with data warehousing.
  n   The architecture and main components of a data warehouse.
  n   The important data flows or processes of a data warehouse.
  n   The main tools and technologies associated with data warehousing.
  n   The issues associated with the integration of a data warehouse and the
      importance of managing metadata.
  n   The concept of a data mart and the main reasons for implementing a data mart.
  n   The main issues associated with the development and management of data marts.
  n   How Oracle supports data warehousing.
We have already noted in earlier chapters that database management systems are pervas-
ive throughout industry, with relational database management systems being the domin-
ant system. These systems have been designed to handle high transaction throughput,
with transactions typically making small changes to the organization’s operational data,
that is, data that the organization requires to handle its day-to-day operations. These types
of system are called Online Transaction Processing (OLTP) systems. The size of OLTP
databases can range from small databases of a few megabytes (Mb), to medium-sized
databases with several gigabytes (Gb), to large databases requiring terabytes (Tb) or even
petabytes (Pb) of storage.
   Corporate decision-makers require access to all the organization’s data, wherever it is
located. To provide comprehensive analysis of the organization, its business, its require-
ments, and any trends, requires access to not only the current values in the database but
also to historical data. To facilitate this type of analysis, the data warehouse has been
created to hold data drawn from several data sources, maintained by different operating
units, together with historical and summary transformations. The data warehouse based on
                                            www.it-ebooks.info


1150  | Chapter 31 z Data Warehousing Concepts
                extended database technology provides the management of the datastore. However,
                decision-makers also require powerful analysis tools. Two main types of analysis tools
                have emerged over the last few years: Online Analytical Processing (OLAP) and data
                mining tools.
                   As data warehousing is such a complex subject, we have devoted four chapters to
                different aspects of data warehousing. In this chapter, we describe the basic concepts asso-
                ciated with data warehousing. In Chapter 32 we describe how to design and build a data
                warehouse and in Chapters 33 and 34 we discuss the important end-user access tools for a
                data warehouse.
                Structure of this Chapter
                In Section 31.1 we outline what data warehousing is and how it evolved, and also describe
                the potential benefits and problems associated with this approach. In Section 31.2 we
                describe the architecture and main components of a data warehouse. In Sections 31.3 and
                31.4 we identify and discuss the important data flows or processes of a data warehouse, and
                the associated tools and technologies of a data warehouse, respectively. In Section 31.5 we
                introduce data marts and the issues associated with the development and management of
                data marts. Finally, in Section 31.6 we present an overview of how Oracle supports a data
                warehouse environment. The examples in this chapter are taken from the DreamHome
                case study described in Section 10.4 and Appendix A.
     31.1       Introduction to Data Warehousing
                In this section we discuss the origin and evolution of the concept of data warehousing.
                We then discuss the main benefits associated with data warehousing. We next identify the
                main characteristics of data warehousing systems in comparison with Online Transaction
                Processing (OLTP) systems. We conclude this section by examining the problems of
                developing and managing a data warehouse.
     31.1.1 The Evolution of Data Warehousing
                Since the 1970s, organizations have mostly focused their investment in new computer
                systems that automate business processes. In this way, organizations gained competitive
                advantage through systems that offered more efficient and cost-effective services to the
                customer. Throughout this period, organizations accumulated growing amounts of data
                stored in their operational databases. However, in recent times, where such systems are
                commonplace, organizations are focusing on ways to use operational data to support
                decision-making, as a means of regaining competitive advantage.
                   Operational systems were never designed to support such business activities and so
                using these systems for decision-making may never be an easy solution. The legacy is that
                                             www.it-ebooks.info


                                                                31.1 Introduction to Data Warehousing | 1151
a typical organization may have numerous operational systems with overlapping and
sometimes contradictory definitions, such as data types. The challenge for an organization
is to turn its archives of data into a source of knowledge, so that a single integrated/
consolidated view of the organization’s data is presented to the user. The concept of a
data warehouse was deemed the solution to meet the requirements of a system capable of
supporting decision-making, receiving data from multiple operational data sources.
Data Warehousing Concepts                                                                        31.1.2
The original concept of a data warehouse was devised by IBM as the ‘information ware-
house’ and presented as a solution for accessing data held in non-relational systems. The
information warehouse was proposed to allow organizations to use their data archives to
help them gain a business advantage. However, due to the sheer complexity and perform-
ance problems associated with the implementation of such solutions, the early attempts at
creating an information warehouse were mostly rejected. Since then, the concept of data
warehousing has been raised several times but it is only in recent years that the potential
of data warehousing is now seen as a valuable and viable solution. The latest and most
successful advocate for data warehousing is Bill Inmon, who has earned the title of ‘father
of data warehousing’ due to his active promotion of the concept.
  Data                A subject-oriented, integrated, time-variant, and non-volatile collec-
  warehousing         tion of data in support of management’s decision-making process.
   In this definition by Inmon (1993), the data is:
n  Subject-oriented as the warehouse is organized around the major subjects of the enter-
   prise (such as customers, products, and sales) rather than the major application areas
   (such as customer invoicing, stock control, and product sales). This is reflected in the
   need to store decision-support data rather than application-oriented data.
n  Integrated because of the coming together of source data from different enterprise-wide
   applications systems. The source data is often inconsistent using, for example, different
   formats. The integrated data source must be made consistent to present a unified view
   of the data to the users.
n  Time-variant because data in the warehouse is only accurate and valid at some point in
   time or over some time interval. The time-variance of the data warehouse is also shown
   in the extended time that the data is held, the implicit or explicit association of time with
   all data, and the fact that the data represents a series of snapshots.
n  Non-volatile as the data is not updated in real time but is refreshed from operational
   systems on a regular basis. New data is always added as a supplement to the database,
   rather than a replacement. The database continually absorbs this new data, incremen-
   tally integrating it with the previous data.
There are numerous definitions of data warehousing, with the earlier definitions focusing
on the characteristics of the data held in the warehouse. Alternative definitions widen the
                                             www.it-ebooks.info


1152  | Chapter 31 z Data Warehousing Concepts
                scope of the definition of data warehousing to include the processing associated with
                accessing the data from the original sources to the delivery of the data to the decision-
                makers (Anahory and Murray, 1997).
                   Whatever the definition, the ultimate goal of data warehousing is to integrate enterprise-
                wide corporate data into a single repository from which users can easily run queries, pro-
                duce reports, and perform analysis. In summary, a data warehouse is data management and
                data analysis technology.
                   In recent years a new term associated with data warehousing has been used, namely
                ‘Data Webhouse’.
                  Data            A distributed data warehouse that is implemented over the Web with
                  Webhouse        no central data repository.
                   The Web is an immense source of behavioral data as individuals interact through
                their Web browsers with remote Web sites. The data generated by this behavior is called
                clickstream. Using a data warehouse on the Web to harness clickstream data has led to
                the development of Data Webhouses. Further discussions on the development of this new
                variation of data warehousing is out with the scope of this book, however the interested
                reader is referred to Kimball et al. (2000).
     31.1.3 Benefits of Data Warehousing
                The successful implementation of a data warehouse can bring major benefits to an
                organization including:
                n  Potential high returns on investment An organization must commit a huge amount of
                   resources to ensure the successful implementation of a data warehouse and the cost
                   can vary enormously from £50,000 to over £10 million due to the variety of technical
                   solutions available. However, a study by the International Data Corporation (IDC) in
                   1996 reported that average three-year returns on investment (ROI) in data warehousing
                   reached 401%, with over 90% of the companies surveyed achieving over 40% ROI, half
                   the companies achieving over 160% ROI, and a quarter with more than 600% ROI
                   (IDC, 1996).
                n  Competitive advantage The huge returns on investment for those companies that have
                   successfully implemented a data warehouse is evidence of the enormous competitive
                   advantage that accompanies this technology. The competitive advantage is gained
                   by allowing decision-makers access to data that can reveal previously unavailable,
                   unknown, and untapped information on, for example, customers, trends, and demands.
                n  Increased productivity of corporate decision-makers Data warehousing improves
                   the productivity of corporate decision-makers by creating an integrated database of
                   consistent, subject-oriented, historical data. It integrates data from multiple incom-
                   patible systems into a form that provides one consistent view of the organization. By
                   transforming data into meaningful information, a data warehouse allows corporate
                   decision-makers to perform more substantive, accurate, and consistent analysis.
                                             www.it-ebooks.info


                                                                 31.1 Introduction to Data Warehousing | 1153
Comparison of OLTP Systems and                                                                   31.1.4
Data Warehousing
A DBMS built for Online Transaction Processing (OLTP) is generally regarded as unsuit-
able for data warehousing because each system is designed with a differing set of require-
ments in mind. For example, OLTP systems are designed to maximize the transaction
processing capacity, while data warehouses are designed to support ad hoc query pro-
cessing. Table 31.1 provides a comparison of the major characteristics of OLTP systems
and data warehousing systems (Singh, 1997).
   An organization will normally have a number of different OLTP systems for business
processes such as inventory control, customer invoicing, and point-of-sale. These systems
generate operational data that is detailed, current, and subject to change. The OLTP sys-
tems are optimized for a high number of transactions that are predictable, repetitive, and
update intensive. The OLTP data is organized according to the requirements of the trans-
actions associated with the business applications and supports the day-to-day decisions of
a large number of concurrent operational users.
   In contrast, an organization will normally have a single data warehouse, which holds
data that is historical, detailed, and summarized to various levels and rarely subject to
change (other than being supplemented with new data). The data warehouse is designed
to support relatively low numbers of transactions that are unpredictable in nature and
require answers to queries that are ad hoc, unstructured, and heuristic. The warehouse data
is organized according to the requirements of potential queries and supports the long-term
strategic decisions of a relatively low number of managerial users.
   Although OLTP systems and data warehouses have different characteristics and are
built with different purposes in mind, these systems are closely related in that the OLTP
systems provide the source data for the warehouse. A major problem of this relationship
is that the data held by the OLTP systems can be inconsistent, fragmented, and subject
Table 31.1    Comparison of OLTP systems and data warehousing systems.
  OLTP systems                            Data warehousing systems
  Holds current data                      Holds historical data
  Stores detailed data                    Stores detailed, lightly, and highly summarized data
  Data is dynamic                         Data is largely static
  Repetitive processing                   Ad hoc, unstructured, and heuristic processing
  High level of transaction throughput    Medium to low level of transaction throughput
  Predictable pattern of usage            Unpredictable pattern of usage
  Transaction-driven                      Analysis driven
  Application-oriented                    Subject-oriented
  Supports day-to-day decisions           Supports strategic decisions
  Serves large number of                  Serves relatively low number of managerial users
  clerical/operational users
                                           www.it-ebooks.info


1154  | Chapter 31 z Data Warehousing Concepts
                to change, containing duplicate or missing entries. As such, the operational data must be
                ‘cleaned up’ before it can be used in the data warehouse. We discuss the tasks associated
                with this process in Section 31.3.1.
                   OLTP systems are not built to quickly answer ad hoc queries. They also tend not to store
                historical data, which is necessary to analyze trends. Basically, OLTP offers large amounts
                of raw data, which is not easily analyzed. The data warehouse allows more complex queries
                to be answered besides just simple aggregations such as, ‘What is the average selling price
                for properties in the major cities of Great Britain?’. The types of queries that a data ware-
                house is expected to answer range from the relatively simple to the highly complex and are
                dependent on the types of end-user access tools used (see Section 31.2.10). Examples of the
                range of queries that the DreamHome data warehouse may be capable of supporting include:
                n  What was the total revenue for Scotland in the third quarter of 2004?
                n  What was the total revenue for property sales for each type of property in Great Britain
                   in 2003?
                n  What are the three most popular areas in each city for the renting of property in 2004
                   and how does this compare with the results for the previous two years?
                n  What is the monthly revenue for property sales at each branch office, compared with
                   rolling 12-monthly prior figures?
                n  What would be the effect on property sales in the different regions of Britain if legal costs
                   went up by 3.5% and Government taxes went down by 1.5% for properties over £100,000?
                n  Which type of property sells for prices above the average selling price for properties in
                   the main cities of Great Britain and how does this correlate to demographic data?
                n  What is the relationship between the total annual revenue generated by each branch
                   office and the total number of sales staff assigned to each branch office?
     31.1.5 Problems of Data Warehousing
                The problems associated with developing and managing a data warehouse are listed in
                Table 31.2 (Greenfield, 1996).
                                          Table 31.2   Problems of data warehousing.
                                          Underestimation of resources for data loading
                                          Hidden problems with source systems
                                          Required data not captured
                                          Increased end-user demands
                                          Data homogenization
                                          High demand for resources
                                          Data ownership
                                          High maintenance
                                          Long-duration projects
                                          Complexity of integration
                                              www.it-ebooks.info


                                                               31.1 Introduction to Data Warehousing | 1155
Underestimation of resources for data loading
Many developers underestimate the time required to extract, clean, and load the data into
the warehouse. This process may account for a significant proportion of the total develop-
ment time, although better data cleansing and management tools should ultimately reduce
the time and effort spent.
Hidden problems with source systems
Hidden problems associated with the source systems feeding the data warehouse may be
identified, possibly after years of being undetected. The developer must decide whether
to fix the problem in the data warehouse and/or fix the source systems. For example, when
entering the details of a new property, certain fields may allow nulls, which may result in
staff entering incomplete property data, even when available and applicable.
Required data not captured
Warehouse projects often highlight a requirement for data not being captured by the
existing source systems. The organization must decide whether to modify the OLTP sys-
tems or create a system dedicated to capturing the missing data. For example, when con-
sidering the DreamHome case study, we may wish to analyze the characteristics of certain
events such as the registering of new clients and properties at each branch office. However,
this is currently not possible as we do not capture the data that the analysis requires such
as the date registered in either case.
Increased end-user demands
After end-users receive query and reporting tools, requests for support from IS staff may
increase rather than decrease. This is caused by an increasing awareness of the users on
the capabilities and value of the data warehouse. This problem can be partially alleviated
by investing in easier-to-use, more powerful tools, or in providing better training for the
users. A further reason for increasing demands on IS staff is that once a data warehouse is
online, it is often the case that the number of users and queries increase together with
requests for answers to more and more complex queries.
Data homogenization
Large-scale data warehousing can become an exercise in data homogenization that lessens
the value of the data. For example, in producing a consolidated and integrated view of the
organization’s data, the warehouse designer may be tempted to emphasize similarities
rather than differences in the data used by different application areas such as property sales
and property renting.
High demand for resources
The data warehouse can use large amounts of disk space. Many relational databases
used for decision-support are designed around star, snowflake, and starflake schemas
                                           www.it-ebooks.info


1156  | Chapter 31 z Data Warehousing Concepts
                (see Chapter 32). These approaches result in the creation of very large fact tables. If there
                are many dimensions to the factual data, the combination of aggregate tables and indexes
                to the fact tables can use up more space than the raw data.
                Data ownership
                Data warehousing may change the attitude of end-users to the ownership of data. Sensitive
                data that was originally viewed and used only by a particular department or business area,
                such as sales or marketing, may now be made accessible to others in the organization.
                High maintenance
                Data warehouses are high maintenance systems. Any reorganization of the business
                processes and the source systems may affect the data warehouse. To remain a valuable
                resource, the data warehouse must remain consistent with the organization that it supports.
                Long-duration projects
                A data warehouse represents a single data resource for the organization. However, the
                building of a warehouse can take up to three years, which is why some organizations are
                building data marts (see Section 31.5). Data marts support only the requirements of a
                particular department or functional area and can therefore be built more rapidly.
                Complexity of integration
                The most important area for the management of a data warehouse is the integration
                capabilities. This means an organization must spend a significant amount of time deter-
                mining how well the various different data warehousing tools can be integrated into the
                overall solution that is needed. This can be a very difficult task, as there are a number of
                tools for every operation of the data warehouse, which must integrate well in order that the
                warehouse works to the organization’s benefit.
     31.2       Data Warehouse Architecture
                In this section we present an overview of the architecture and major components of a data
                warehouse (Anahory and Murray, 1997). The processes, tools, and technologies associated
                with data warehousing are described in more detail in the following sections of this chapter.
                The typical architecture of a data warehouse is shown in Figure 31.1.
     31.2.1 Operational Data
                The source of data for the data warehouse is supplied from:
                n  Mainframe operational data held in first generation hierarchical and network databases.
                   It is estimated that the majority of corporate operational data is held in these systems.
                                             www.it-ebooks.info


                                                                   31.2 Data Warehouse Architecture | 1157
Figure 31.1   Typical architecture of a data warehouse.
n  Departmental data held in proprietary file systems such as VSAM, RMS, and relational
   DBMSs such as Informix and Oracle.
n  Private data held on workstations and private servers.
n  External systems such as the Internet, commercially available databases, or databases
   associated with an organization’s suppliers or customers.
Operational Data Store                                                                        31.2.2
An Operational Data Store (ODS) is a repository of current and integrated operational data
used for analysis. It is often structured and supplied with data in the same way as the
data warehouse, but may in fact act simply as a staging area for data to be moved into the
warehouse.
   The ODS is often created when legacy operational systems are found to be incapable
of achieving reporting requirements. The ODS provides users with the ease of use of a
relational database while remaining distant from the decision support functions of the
data warehouse.
                                                www.it-ebooks.info


1158  | Chapter 31 z Data Warehousing Concepts
                   Building an ODS can be a helpful step towards building a data warehouse because an
                ODS can supply data that has been already extracted from the source systems and cleaned.
                This means that the remaining work of integrating and restructuring the data for the data
                warehouse is simplified (see Section 32.3).
     31.2.3 Load Manager
                The load manager (also called the frontend component) performs all the operations
                associated with the extraction and loading of data into the warehouse. The data may be
                extracted directly from the data sources or more commonly from the operational data store.
                The operations performed by the load manager may include simple transformations of the
                data to prepare the data for entry into the warehouse. The size and complexity of this com-
                ponent will vary between data warehouses and may be constructed using a combination
                of vendor data loading tools and custom-built programs.
     31.2.4 Warehouse Manager
                The warehouse manager performs all the operations associated with the management of
                the data in the warehouse. This component is constructed using vendor data management
                tools and custom-built programs. The operations performed by the warehouse manager
                include:
                n  analysis of data to ensure consistency;
                n  transformation and merging of source data from temporary storage into data warehouse
                   tables;
                n  creation of indexes and views on base tables;
                n  generation of denormalizations (if necessary);
                n  generation of aggregations (if necessary);
                n  backing-up and archiving data.
                In some cases, the warehouse manager also generates query profiles to determine which
                indexes and aggregations are appropriate. A query profile can be generated for each user,
                group of users, or the data warehouse and is based on information that describes the char-
                acteristics of the queries such as frequency, target table(s), and size of result sets.
     31.2.5 Query Manager
                The query manager (also called the backend component) performs all the operations
                associated with the management of user queries. This component is typically constructed
                using vendor end-user data access tools, data warehouse monitoring tools, database
                facilities, and custom-built programs. The complexity of the query manager is determined
                by the facilities provided by the end-user access tools and the database. The operations
                                             www.it-ebooks.info


                                                                     31.2 Data Warehouse Architecture | 1159
performed by this component include directing queries to the appropriate tables and
scheduling the execution of queries. In some cases, the query manager also generates
query profiles to allow the warehouse manager to determine which indexes and aggrega-
tions are appropriate.
Detailed Data                                                                                   31.2.6
This area of the warehouse stores all the detailed data in the database schema. In most
cases, the detailed data is not stored online but is made available by aggregating the data
to the next level of detail. However, on a regular basis, detailed data is added to the ware-
house to supplement the aggregated data.
Lightly and Highly Summarized Data                                                              31.2.7
This area of the warehouse stores all the predefined lightly and highly summarized (aggregated)
data generated by the warehouse manager. This area of the warehouse is transient as it will
be subject to change on an ongoing basis in order to respond to changing query profiles.
   The purpose of summary information is to speed up the performance of queries.
Although there are increased operational costs associated with initially summarizing the
data, this is offset by removing the requirement to continually perform summary opera-
tions (such as sorting or grouping) in answering user queries. The summary data is updated
continuously as new data is loaded into the warehouse.
Archive/Backup Data                                                                             31.2.8
This area of the warehouse stores the detailed and summarized data for the purposes of
archiving and backup. Even although summary data is generated from detailed data, it
may be necessary to backup online summary data if this data is kept beyond the retention
period for detailed data. The data is transferred to storage archives such as magnetic tape
or optical disk.
Metadata                                                                                        31.2.9
This area of the warehouse stores all the metadata (data about data) definitions used by all
the processes in the warehouse. Metadata is used for a variety of purposes including:
n  the extraction and loading processes – metadata is used to map data sources to a
   common view of the data within the warehouse;
n  the warehouse management process – metadata is used to automate the production of
   summary tables;
n  as part of the query management process – metadata is used to direct a query to the most
   appropriate data source.
                                             www.it-ebooks.info


1160 | Chapter 31 z Data Warehousing Concepts
               The structure of metadata differs between each process, because the purpose is different.
               This means that multiple copies of metadata describing the same data item are held within
               the data warehouse. In addition, most vendor tools for copy management and end-user
               data access use their own versions of metadata. Specifically, copy management tools use
               metadata to understand the mapping rules to apply in order to convert the source data into
               a common form. End-user access tools use metadata to understand how to build a query.
               The management of metadata within the data warehouse is a very complex task that should
               not be underestimated. The issues associated with the management of metadata in a data
               warehouse are discussed in Section 31.4.3.
   31.2.10 End-User Access Tools
               The principal purpose of data warehousing is to provide information to business users
               for strategic decision-making. These users interact with the warehouse using end-user
               access tools. The data warehouse must efficiently support ad hoc and routine analysis.
               High performance is achieved by pre-planning the requirements for joins, summations,
               and periodic reports by end-users.
                  Although the definitions of end-user access tools can overlap, for the purpose of this
               discussion, we categorize these tools into five main groups (Berson and Smith, 1997):
               n  reporting and query tools;
               n  application development tools;
               n  Executive Information System (EIS) tools;
               n  Online Analytical Processing (OLAP) tools;
               n  data mining tools.
               Reporting and query tools
               Reporting tools include production reporting tools and report writers. Production report-
               ing tools are used to generate regular operational reports or support high-volume batch
               jobs, such as customer orders/invoices and staff pay cheques. Report writers, on the other
               hand, are inexpensive desktop tools designed for end-users.
                  Query tools for relational data warehouses are designed to accept SQL or generate
               SQL statements to query data stored in the warehouse. These tools shield end-users from
               the complexities of SQL and database structures by including a meta-layer between users
               and the database. The meta-layer is the software that provides subject-oriented views of
               a database and supports ‘point-and-click’ creation of SQL. An example of a query tool is
               Query-By-Example (QBE). The QBE facility of Microsoft Office Access DBMS was
               demonstrated in Chapter 7. Query tools are popular with users of business applications
               such as demographic analysis and customer mailing lists. However, as questions become
               increasingly complex, these tools may rapidly become inefficient.
               Application development tools
               The requirements of the end-users may be such that the built-in capabilities of reporting
               and query tools are inadequate either because the required analysis cannot be performed
                                           www.it-ebooks.info


                                                                  31.3 Data Warehouse Data Flows | 1161
or because the user interaction requires an unreasonably high level of expertise by the
user. In this situation, user access may require the development of in-house applications
using graphical data access tools designed primarily for client–server environments. Some
of these application development tools integrate with popular OLAP tools, and can access
all major database systems, including Oracle, Sybase, and Informix.
Executive information system (EIS) tools
Executive information systems, more recently referred to as ‘everybody’s information
systems’, were originally developed to support high-level strategic decision-making. How-
ever, the focus of these systems widened to include support for all levels of management.
EIS tools were originally associated with mainframes enabling users to build customized,
graphical decision-support applications to provide an overview of the organization’s data
and access to external data sources.
   Currently, the demarcation between EIS tools and other decision-support tools is even
more vague as EIS developers add additional query facilities and provide custom-built
applications for business areas such as sales, marketing, and finance.
Online Analytical Processing (OLAP) tools
Online Analytical Processing (OLAP) tools are based on the concept of multi-dimensional
databases and allow a sophisticated user to analyze the data using complex, multi-
dimensional views. Typical business applications for these tools include assessing the
effectiveness of a marketing campaign, product sales forecasting, and capacity plan-
ning. These tools assume that the data is organized in a multi-dimensional model
supported by a special multi-dimensional database (MDDB) or by a relational database
designed to enable multi-dimensional queries. We discuss OLAP tools in more detail in
Chapter 33.
Data mining tools
Data mining is the process of discovering meaningful new correlations, patterns, and
trends by mining large amounts of data using statistical, mathematical, and artificial
intelligence (AI) techniques. Data mining has the potential to supersede the capabilities of
OLAP tools, as the major attraction of data mining is its ability to build predictive rather
than retrospective models. We discuss data mining in more detail in Chapter 34.
Data Warehouse Data Flows                                                                    31.3
In this section we examine the activities associated with the processing (or flow) of data
within a data warehouse. Data warehousing focuses on the management of five primary
data flows, namely the inflow, upflow, downflow, outflow, and metaflow (Hackathorn,
1995). The data flows within a data warehouse are shown in Figure 31.2. The processes
associated with each data flow include:
                                           www.it-ebooks.info


1162     |  Chapter 31 z Data Warehousing Concepts
Figure 31.2 Information flows of a data warehouse.
                      n   Inflow         Extraction, cleansing, and loading of the source data.
                      n   Upflow         Adding value to the data in the warehouse through summarizing, pack-
                                         aging, and distribution of the data.
                      n   Downflow       Archiving and backing-up the data in the warehouse.
                      n   Outflow        Making the data available to end-users.
                      n   Metaflow       Managing the metadata.
       31.3.1 Inflow
                        Inflow      The processes associated with the extraction, cleansing, and loading of the
                                    data from the source systems into the data warehouse.
                      The inflow is concerned with taking data from the source systems to load into the data
                      warehouse. Alternatively, the data may be first loaded into the operational data store
                                                    www.it-ebooks.info


                                                                    31.3 Data Warehouse Data Flows  | 1163
(ODS) (see Section 31.2.2) before being transferred to the data warehouse. As the source
data is generated predominately by OLTP systems, the data must be reconstructed for the
purposes of the data warehouse. The reconstruction of data involves:
n  cleansing dirty data;
n  restructuring data to suit the new requirements of the data warehouse including, for
   example, adding and/or removing fields, and denormalizing data;
n  ensuring that the source data is consistent with itself and with the data already in the
   warehouse.
To effectively manage the inflow, mechanisms must be identified to determine when to
start extracting the data to carry out the necessary transformations and to undertake con-
sistency checks. When extracting data from the source systems, it is important to ensure
that the data is in a consistent state to generate a single, consistent view of the corporate
data. The complexity of the extraction process is determined by the extent to which the
source systems are ‘in tune’ with one another.
   Once the data is extracted, the data is usually loaded into a temporary store for the
purposes of cleansing and consistency checking. As this process is complex, it is
important for it to be fully automated and to have the ability to report when problems
and failures occur. Commercial tools are available to support the management of the
inflow. However, unless the process is relatively straightforward, the tools may require
customization.
Upflow                                                                                         31.3.2
  Upflow      The processes associated with adding value to the data in the warehouse
              through summarizing, packaging, and distribution of the data.
The activities associated with the upflow include:
n  Summarizing the data by selecting, projecting, joining, and grouping relational data
   into views that are more convenient and useful to the end-users. Summarizing extends
   beyond simple relational operations to involve sophisticated statistical analysis
   including identifying trends, clustering, and sampling the data.
n  Packaging the data by converting the detailed or summarized data into more useful
   formats, such as spreadsheets, text documents, charts, other graphical presentations,
   private databases, and animation.
n  Distributing the data to appropriate groups to increase its availability and accessibility.
While adding value to the data, consideration must also be given to support the perform-
ance requirements of the data warehouse and to minimize the ongoing operational costs.
These requirements essentially pull the design in opposing directions, forcing restructur-
ing to improve query performance or to lower operational costs. In other words, the data
warehouse administrator must identify the most appropriate database design to meet all
requirements, which often necessitates a degree of compromise.
                                            www.it-ebooks.info


1164  | Chapter 31 z Data Warehousing Concepts
     31.3.3 Downflow
                  Downflow        The processes associated with archiving and backing-up of data in the
                                  warehouse.
                Archiving old data plays an important role in maintaining the effectiveness and perform-
                ance of the warehouse by transferring the older data of limited value to a storage archive
                such as magnetic tape or optical disk. However, if the correct partitioning scheme is
                selected for the database, the amount of data online should not affect performance.
                   Partitioning is a useful design option for very large databases that enables the frag-
                mentation of a table storing enormous numbers of records into several smaller tables. The
                rule for the partitioning a given table can be based on characteristics of the data such as
                timespan or area of the country. For example, the PropertySale table of DreamHome could
                be partitioned according to the countries of the UK.
                   The downflow of data includes the processes to ensure that the current state of the data
                warehouse can be rebuilt following data loss, or software/hardware failures. Archived data
                should be stored in a way that allows the re-establishment of the data in the warehouse,
                when required.
     31.3.4 Outflow
                  Outflow      The processes associated with making the data available to the
                               end-users.
                The outflow is where the real value of warehousing is realized by the organization.
                This may require re-engineering the business processes to achieve competitive advantage
                (Hackathorn, 1995). The two key activities involved in the outflow include:
                n  Accessing, which is concerned with satisfying the end-users’ requests for the data they
                   need. The main issue is to create an environment so that users can effectively use
                   the query tools to access the most appropriate data source. The frequency of user
                   accesses can vary from ad hoc, to routine, to real-time. It is important to ensure that the
                   system’s resources are used in the most effective way in scheduling the execution
                   of user queries.
                n  Delivering, which is concerned with proactively delivering information to the end-users’
                   workstations and is referred to as a type of ‘publish-and-subscribe’ process. The
                   warehouse publishes various ‘business objects’ that are revised periodically by monitor-
                   ing usage patterns. Users subscribe to the set of business objects that best meets their
                   needs.
                An important issue in managing the outflow is the active marketing of the data warehouse
                to users, which will contribute to its overall impact on an organization’s operations. There
                are additional operational activities in managing the outflow including directing queries to
                                             www.it-ebooks.info


                                                    31.4 Data Warehousing Tools and Technologies | 1165
the appropriate target table(s) and capturing information on the query profiles associated
with user groups to determine which aggregations to generate.
   Data warehouses that contain summary data potentially provide a number of distinct
data sources to respond to a specific query including the detailed data itself and any num-
ber of aggregations that satisfy the query’s data needs. However, the performance of the
query will vary considerably depending on the characteristics of the target data, the most
obvious being the volume of data to be read. As part of managing the outflow, the system
must determine the most efficient way to answer a query.
Metaflow                                                                                    31.3.5
  Metaflow       The processes associated with the management of the metadata.
The previous flows describe the management of the data warehouse with regard to how the
data moves in and out of the warehouse. Metaflow is the process that moves metadata
(data about the other flows). Metadata is a description of the data contents of the data
warehouse, what is in it, where it came from originally, and what has been done to it by
way of cleansing, integrating, and summarizing. We discuss issues associated with the
management of metadata in a data warehouse in Section 31.4.3.
   To respond to changing business needs, legacy systems are constantly changing. There-
fore, the warehouse involves responding to these continuous changes, which must reflect
the changes to the source legacy systems and the changing business environment. The
metaflow (metadata) must be continuously updated with these changes.
Data Warehousing Tools and Technologies                                                      31.4
In this section we examine the tools and technologies associated with building and
managing a data warehouse and, in particular, we focus on the issues associated with the
integration of these tools. For more information on data warehousing tools and tech-
nologies, the interested reader is referred to Berson and Smith (1997).
Extraction, Cleansing, and Transformation Tools                                             31.4.1
Selecting the correct extraction, cleansing, and transformation tools are critical steps in
the construction of a data warehouse. There are an increasing number of vendors that are
focused on fulfilling the requirements of data warehouse implementations as opposed
to simply moving data between hardware platforms. The tasks of capturing data from
a source system, cleansing and transforming it, and then loading the results into a target
system can be carried out either by separate products, or by a single integrated solution.
Integrated solutions fall into one of the following categories:
                                            www.it-ebooks.info


1166  | Chapter 31 z Data Warehousing Concepts
                n  code generators;
                n  database data replication tools;
                n  dynamic transformation engines.
                Code generators
                Code generators create customized 3GL/4GL transformation programs based on source and
                target data definitions. The main issue with this approach is the management of the large
                number of programs required to support a complex corporate data warehouse. Vendors
                recognize this issue and some are developing management components employing tech-
                niques such as workflow methods and automated scheduling systems.
                Database data replication tools
                Database data replication tools employ database triggers or a recovery log to capture
                changes to a single data source on one system and apply the changes to a copy of the
                source data located on a different system (see Chapter 24). Most replication products do
                not support the capture of changes to non-relational files and databases, and often do not
                provide facilities for significant data transformation and enhancement. These tools can be
                used to rebuild a database following failure or to create a database for a data mart (see
                Section 31.5), provided that the number of data sources is small and the level of data
                transformation is relatively simple.
                Dynamic transformation engines
                Rule-driven dynamic transformation engines capture data from a source system at user-
                defined intervals, transform the data, and then send and load the results into a target envir-
                onment. To date, most products support only relational data sources, but products are now
                emerging that handle non-relational source files and databases.
     31.4.2 Data Warehouse DBMS
                There are few integration issues associated with the data warehouse database. Due to the
                maturity of such products, most relational databases will integrate predictably with other
                types of software. However, there are issues associated with the potential size of the data
                warehouse database. Parallelism in the database becomes an important issue, as well as the
                usual issues such as performance, scalability, availability, and manageability, which must
                all be taken into consideration when choosing a DBMS. We first identify the requirements
                for a data warehouse DBMS and then discuss briefly how the requirements of data ware-
                housing are supported by parallel technologies.
                Requirements for data warehouse DBMS
                The specialized requirements for a relational DBMS suitable for data warehousing are
                published in a White Paper (Red Brick Systems, 1996) and are listed in Table 31.3.
                                              www.it-ebooks.info


                                                     31.4 Data Warehousing Tools and Technologies | 1167
                 Table 31.3   The requirements for a data warehouse RDBMS.
                 Load performance
                 Load processing
                 Data quality management
                 Query performance
                 Terabyte scalability
                 Mass user scalability
                 Networked data warehouse
                 Warehouse administration
                 Integrated dimensional analysis
                 Advanced query functionality
Load performance
Data warehouses require incremental loading of new data on a periodic basis within
narrow time windows. Performance of the load process should be measured in hundreds
of millions of rows or gigabytes of data per hour and there should be no maximum limit
that constrains the business.
Load processing
Many steps must be taken to load new or updated data into the data warehouse including
data conversions, filtering, reformatting, integrity checks, physical storage, indexing, and
metadata update. Although each step may in practice be atomic, the load process should
appear to execute as a single, seamless unit of work.
Data quality management
The shift to fact-based management demands the highest data quality. The warehouse
must ensure local consistency, global consistency, and referential integrity despite ‘dirty’
sources and massive database sizes. While loading and preparation are necessary steps,
they are not sufficient. The ability to answer end-users’ queries is the measure of success
for a data warehouse application. As more questions are answered, analysts tend to ask
more creative and complex questions.
Query performance
Fact-based management and ad hoc analysis must not be slowed or inhibited by the
performance of the data warehouse RDBMS. Large, complex queries for key business
operations must complete in reasonable time periods.
Terabyte scalability
Data warehouse sizes are growing at enormous rates with sizes ranging from a few to
hundreds of gigabytes to terabyte-sized (1012 bytes) and petabyte-sized (1015 bytes).
                                            www.it-ebooks.info


1168 | Chapter 31 z Data Warehousing Concepts
               The RDBMS must not have any architectural limitations to the size of the database and
               should support modular and parallel management. In the event of failure, the RDBMS should
               support continued availability, and provide mechanisms for recovery. The RDBMS must
               support mass storage devices such as optical disk and hierarchical storage management
               devices. Lastly, query performance should not be dependent on the size of the database,
               but rather on the complexity of the query.
               Mass user scalability
               Current thinking is that access to a data warehouse is limited to relatively low numbers
               of managerial users. This is unlikely to remain true as the value of data warehouses is
               realized. It is predicted that the data warehouse RDBMS should be capable of supporting
               hundreds, or even thousands, of concurrent users while maintaining acceptable query
               performance.
               Networked data warehouse
               Data warehouse systems should be capable of cooperating in a larger network of data ware-
               houses. The data warehouse must include tools that coordinate the movement of subsets
               of data between warehouses. Users should be able to look at, and work with, multiple data
               warehouses from a single client workstation.
               Warehouse administration
               The very-large scale and time-cyclic nature of the data warehouse demands administrat-
               ive ease and flexibility. The RDBMS must provide controls for implementing resource
               limits, chargeback accounting to allocate costs back to users, and query prioritization to
               address the needs of different user classes and activities. The RDBMS must also provide
               for workload tracking and tuning so that system resources may be optimized for maximum
               performance and throughput. The most visible and measurable value of implementing
               a data warehouse is evidenced in the uninhibited, creative access to data it provides for
               end-users.
               Integrated dimensional analysis
               The power of multi-dimensional views is widely accepted, and dimensional support
               must be inherent in the warehouse RDBMS to provide the highest performance for
               relational OLAP tools (see Chapter 33). The RDBMS must support fast, easy creation of
               pre-computed summaries common in large data warehouses, and provide maintenance
               tools to automate the creation of these pre-computed aggregates. Dynamic calculation of
               aggregates should be consistent with the interactive performance needs of the end-user.
               Advanced query functionality
               End-users require advanced analytical calculations, sequential and comparative analysis,
               and consistent access to detailed and summarized data. Using SQL in a client–server
               ‘point-and-click’ tool environment may sometimes be impractical or even impossible
               due to the complexity of the users’ queries. The RDBMS must provide a complete and
               advanced set of analytical operations.
                                             www.it-ebooks.info


                                                      31.4 Data Warehousing Tools and Technologies  | 1169
Parallel DBMSs
Data warehousing requires the processing of enormous amounts of data and parallel data-
base technology offers a solution to providing the necessary growth in performance. The
success of parallel DBMSs depends on the efficient operation of many resources includ-
ing processors, memory, disks, and network connections. As data warehousing grows
in popularity, many vendors are building large decision-support DBMSs using parallel
technologies. The aim is to solve decision-support problems using multiple nodes work-
ing on the same problem. The major characteristics of parallel DBMSs are scalability,
operability, and availability.
   The parallel DBMS performs many database operations simultaneously, splitting
individual tasks into smaller parts so that tasks can be spread across multiple processors.
Parallel DBMSs must be capable of running parallel queries. In other words, they must
be able to decompose large complex queries into subqueries, run the separate subqueries
simultaneously, and reassemble the results at the end. The capability of such DBMSs must
also include parallel data loading, table scanning, and data archiving and backup. There
are two main parallel hardware architectures commonly used as database server platforms
for data warehousing:
n  Symmetric Multi-Processing (SMP) – a set of tightly coupled processors that share
   memory and disk storage;
n  Massively Parallel Processing (MPP) – a set of loosely coupled processors, each of
   which has its own memory and disk storage.
The SMP and MPP parallel architectures were described in detail in Section 22.1.1.
Data Warehouse Metadata                                                                        31.4.3
There are many issues associated with data warehouse integration, however in this section
we focus on the integration of metadata, that is ‘data about data’ (Darling, 1996). The
management of the metadata in the warehouse is an extremely complex and difficult task.
Metadata is used for a variety of purposes and the management of metadata is a critical
issue in achieving a fully integrated data warehouse.
   The major purpose of metadata is to show the pathway back to where the data began,
so that the warehouse administrators know the history of any item in the warehouse.
However, the problem is that metadata has several functions within the warehouse that
relates to the processes associated with data transformation and loading, data warehouse
management, and query generation (see Section 31.2.9).
   The metadata associated with data transformation and loading must describe the
source data and any changes that were made to the data. For example, for each source
field there should be a unique identifier, original field name, source data type, and original
location including the system and object name, along with the destination data type and
destination table name. If the field is subject to any transformations such as a simple field
type change to a complex set of procedures and functions, this should also be recorded.
   The metadata associated with data management describes the data as it is stored in the
warehouse. Every object in the database needs to be described including the data in each
                                            www.it-ebooks.info


1170 | Chapter 31 z Data Warehousing Concepts
               table, index, and view, and any associated constraints. This information is held in the
               DBMS system catalog, however, there are additional requirements for the purposes of
               the warehouse. For example, metadata should also describe any fields associated with
               aggregations, including a description of the aggregation that was performed. In addition,
               table partitions should be described including information on the partition key, and the
               data range associated with that partition.
                  The metadata described above is also required by the query manager to generate appro-
               priate queries. In turn, the query manager generates additional metadata about the queries
               that are run, which can be used to generate a history on all the queries and a query profile
               for each user, group of users, or the data warehouse. There is also metadata associated
               with the users of queries that includes, for example, information describing what the term
               ‘price’ or ‘customer’ means in a particular database and whether the meaning has changed
               over time.
               Synchronizing metadata
               The major integration issue is how to synchronize the various types of metadata used
               throughout the data warehouse. The various tools of a data warehouse generate and use
               their own metadata, and to achieve integration, we require that these tools are capable of
               sharing their metadata. The challenge is to synchronize metadata between different prod-
               ucts from different vendors using different metadata stores. For example, it is necessary
               to identify the correct item of metadata at the right level of detail from one product and
               map it to the appropriate item of metadata at the right level of detail in another product,
               then sort out any coding differences between them. This has to be repeated for all other
               metadata that the two products have in common. Further, any changes to the metadata
               (or even meta-metadata), in one product needs to be conveyed to the other product. The
               task of synchronizing two products is highly complex, and therefore repeating this process
               for six or more products that make up the data warehouse can be resource intensive.
               However, integration of the metadata must be achieved.
                  In the beginning there were two major standards for metadata and modeling in the
               areas of data warehousing and component-based development proposed by the Meta
               Data Coalition (MDC) and the Object Management Group (OMG). However, these two
               industry organizations jointly announced that the MDC would merge into the OMG. As
               a result, the MDC discontinued independent operations and work continued in the OMG
               to integrate the two standards.
                  The merger of MDC into the OMG marked an agreement of the major data ware-
               housing and metadata vendors to converge on one standard, incorporating the best of the
               MDC’s Open Information Model (OIM) with the best of the OMG’s Common Warehouse
               Metamodel (CWM). This work is now complete and the resulting specification issued by
               the OMG as the next version of the CWM is discussed in Section 27.1.3. A single stand-
               ard allows users to exchange metadata between different products from different vendors
               freely.
                  The OMG’s CWM builds on various standards, including OMG’s UML (Unified
               Modeling Language), XMI (XML Metadata Interchange), and MOF (Meta Object
               Facility), and on the MDC’s OIM. The CWM was developed by a number of companies,
               including IBM, Oracle, Unisys, Hyperion, Genesis, NCR, UBS, and Dimension EDI.
                                             www.it-ebooks.info


                                                                                    31.5 Data Marts | 1171
Administration and Management Tools                                                           31.4.4
A data warehouse requires tools to support the administration and management of such
a complex environment. These tools are relatively scarce, especially those that are well
integrated with the various types of metadata and the day-to-day operations of the data
warehouse. The data warehouse administration and management tools must be capable of
supporting the following tasks:
n  monitoring data loading from multiple sources;
n  data quality and integrity checks;
n  managing and updating metadata;
n  monitoring database performance to ensure efficient query response times and resource
   utilization;
n  auditing data warehouse usage to provide user chargeback information;
n  replicating, subsetting, and distributing data;
n  maintaining efficient data storage management;
n  purging data;
n  archiving and backing-up data;
n  implementing recovery following failure;
n  security management.
Data Marts                                                                                     31.5
Accompanying the rapid emergence of data warehouses is the related concept of data
marts. In this section we describe what data marts are, the reasons for building data marts,
and the issues associated with the development and use of data marts.
  Data      A subset of a data warehouse that supports the requirements of a particular
  mart      department or business function.
   A data mart holds a subset of the data in a data warehouse normally in the form of
summary data relating to a particular department or business function. The data mart can
be standalone or linked centrally to the corporate data warehouse. As a data warehouse
grows larger, the ability to serve the various needs of the organization may be comprom-
ised. The popularity of data marts stems from the fact that corporate-wide data warehouses
are proving difficult to build and use. The typical architecture for a data warehouse and
associated data mart is shown in Figure 31.3. The characteristics that differentiate data
marts and data warehouses include:
n  a data mart focuses on only the requirements of users associated with one department
   or business function;
n  data marts do not normally contain detailed operational data, unlike data warehouses;
                                            www.it-ebooks.info


1172     |  Chapter 31 z Data Warehousing Concepts
Figure 31.3  Typical data warehouse and data mart architecture.
                                                    www.it-ebooks.info


                                                                                      31.5 Data Marts | 1173
n  as data marts contain less data compared with data warehouses, data marts are more
   easily understood and navigated.
   There are several approaches to building data marts. One approach is to build several
data marts with a view to the eventual integration into a warehouse; another approach is
to build the infrastructure for a corporate data warehouse while at the same time building
one or more data marts to satisfy immediate business needs.
   Data mart architectures can be built as two-tier or three-tier database applications. The
data warehouse is the optional first tier (if the data warehouse provides the data for the
data mart), the data mart is the second tier, and the end-user workstation is the third tier,
as shown in Figure 31.3. Data is distributed among the tiers.
Reasons for Creating a Data Mart                                                                31.5.1
There are many reasons for creating a data mart, which include:
n  To give users access to the data they need to analyze most often.
n  To provide data in a form that matches the collective view of the data by a group of
   users in a department or business function.
n  To improve end-user response time due to the reduction in the volume of data to be
   accessed.
n  To provide appropriately structured data as dictated by the requirements of end-user
   access tools such as Online Analytical Processing (OLAP) and data mining tools, which
   may require their own internal database structures. In practice, these tools often create
   their own data mart designed to support their specific functionality.
n  Data marts normally use less data so tasks such as data cleansing, loading, transforma-
   tion, and integration are far easier, and hence implementing and setting up a data mart
   is simpler than establishing a corporate data warehouse.
n  The cost of implementing data marts is normally less than that required to establish a
   data warehouse.
n  The potential users of a data mart are more clearly defined and can be more easily targeted
   to obtain support for a data mart project rather than a corporate data warehouse project.
Data Marts Issues                                                                               31.5.2
The issues associated with the development and management of data marts are listed in
Table 31.4 (Brooks, 1997).
Data mart functionality
The capabilities of data marts have increased with the growth in their popularity. Rather
than being simply small, easy-to-access databases, some data marts must now be scalable
to hundreds of gigabytes (Gb), and provide sophisticated analysis using Online Analytical
                                            www.it-ebooks.info


1174 | Chapter 31 z Data Warehousing Concepts
                                      Table 31.4   The issues associated with data marts.
                                      Data mart functionality
                                      Data mart size
                                      Data mart load performance
                                      Users access to data in multiple data marts
                                      Data mart Internet/intranet access
                                      Data mart administration
                                      Data mart installation
               Processing (OLAP) and/or data mining tools. Further, hundreds of users must be capable
               of remotely accessing the data mart. The complexity and size of some data marts are
               matching the characteristics of small-scale corporate data warehouses.
               Data mart size
               Users expect faster response times from data marts than from data warehouses, however,
               performance deteriorates as data marts grow in size. Several vendors of data marts are
               investigating ways to reduce the size of data marts to gain improvements in perform-
               ance. For example, dynamic dimensions allow aggregations to be calculated on demand
               rather than pre-calculated and stored in the multi-dimensional database (MDDB) cube
               (see Chapter 33).
               Data mart load performance
               A data mart has to balance two critical components: end-user response time and data
               loading performance. A data mart designed for fast user response will have a large
               number of summary tables and aggregate values. Unfortunately, the creation of such tables
               and values greatly increases the time of the load procedure. Vendors are investigating
               improvements in the load procedure by providing indexes that automatically and con-
               tinually adapt to the data being processed or by supporting incremental database updating
               so that only cells affected by the change are updated and not the entire MDDB structure.
               Users’ access to data in multiple data marts
               One approach is to replicate data between different data marts or, alternatively, build
               virtual data marts. Virtual data marts are views of several physical data marts or the
               corporate data warehouse tailored to meet the requirements of specific groups of users.
               Commercial products that manage virtual data marts are available.
               Data mart Internet/Intranet access
               Internet/Intranet technology offers users low-cost access to data marts and the data
               warehouse using Web browsers such as Netscape Navigator and Microsoft Internet
                                            www.it-ebooks.info


                                                              31.6 Data Warehousing Using Oracle   | 1175
Explorer. Data mart Internet/Intranet products normally sit between a Web server and the
data analysis product. Vendors are developing products with increasingly advanced Web
capabilities. These products include Java and ActiveX capabilities. We discussed Web and
DBMS integration in detail in Chapter 29.
Data mart administration
As the number of data marts in an organization increases, so does the need to centrally
manage and coordinate data mart activities. Once data is copied to data marts, data can
become inconsistent as users alter their own data marts to allow them to analyze data in
different ways. Organizations cannot easily perform administration of multiple data marts,
giving rise to issues such as data mart versioning, data and metadata consistency and
integrity, enterprise-wide security, and performance tuning. Data mart administrative tools
are commercially available.
Data mart installation
Data marts are becoming increasingly complex to build. Vendors are offering products
referred to as ‘data marts in a box’ that provide a low-cost source of data mart tools.
Data Warehousing Using Oracle                                                                  31.6
In Chapter 8 we provided a general overview of the major features of the Oracle DBMS.
In this section we describe the features of Oracle9i Enterprise Edition that are specifically
designed to improve performance and manageability for the data warehouse (Oracle
Corporation, 2004f).
Oracle9i                                                                                      31.6.1
Oracle9i Enterprise Edition is one of the leading relational DBMS for data ware-
housing. Oracle has achieved this success by focusing on basic, core requirements for data
warehousing: performance, scalability, and manageability. Data warehouses store larger
volumes of data, support more users, and require faster performance, so that these core
requirements remain key factors in the successful implementation of data warehouses.
However, Oracle goes beyond these core requirements and is the first true ‘data warehouse
platform’. Data warehouse applications require specialized processing techniques to allow
support for complex, ad hoc queries running against large amounts of data. To address
these special requirements, Oracle offers a variety of query processing techniques, sophis-
ticated query optimization to choose the most efficient data access path, and a scalable
architecture that takes full advantage of all parallel hardware configurations. Successful
data warehouse applications rely on superior performance when accessing the enormous
amounts of stored data. Oracle provides a rich variety of integrated indexing schemes,
join methods, and summary management features, to deliver answers quickly to data
                                           www.it-ebooks.info


1176 | Chapter 31 z Data Warehousing Concepts
               warehouse users. Oracle also addresses applications that have mixed workloads and where
               administrators want to control which users, or groups of users, have priority when execut-
               ing transactions or queries. In this section we provide an overview of the main features
               of Oracle, which are particularly aimed at supporting data warehousing applications.
               These features include:
               n  summary management;
               n  analytical functions;
               n  bitmapped indexes;
               n  advanced join methods;
               n  sophisticated SQL optimizer;
               n  resource management.
               Summary management
               In a data warehouse application, users often issue queries that summarize detail data by
               common dimensions, such as month, product, or region. Oracle provides a mechanism for
               storing multiple dimensions and summary calculations on a table. Thus, when a query
               requests a summary of detail records, the query is transparently re-written to access the
               stored aggregates rather than summing the detail records every time the query is issued.
               This results in dramatic improvements in query performance. These summaries are auto-
               matically maintained from data in the base tables. Oracle also provides summary advisory
               functions that assist database administrators in choosing which summary tables are the
               most effective, depending on actual workload and schema statistics. Oracle Enterprise
               Manager supports the creation and management of materialized views and related dimen-
               sions and hierarchies via a graphical interface, greatly simplifying the management of
               materialized views.
               Analytical functions
               Oracle9i includes a range of SQL functions for business intelligence and data warehous-
               ing applications. These functions are collectively called ‘analytical functions’, and they
               provide improved performance and simplified coding for many business analysis queries.
               Some examples of the new capabilities are:
               n  ranking (for example, who are the top ten sales reps in each region of Great Britain?);
               n  moving aggregates (for example, what is the three-month moving average of property
                  sales?);
               n  other functions including cumulative aggregates, lag/lead expressions, period-over-period
                  comparisons, and ratio-to-report.
               Oracle also includes the CUBE and ROLLUP operators for OLAP analysis, via SQL.
               These analytical and OLAP functions significantly extend the capabilities of Oracle for
               analytical applications (see Chapter 33).
                                            www.it-ebooks.info


                                                                 31.6 Data Warehousing Using Oracle | 1177
Bitmapped indexes
Bitmapped indexes deliver performance benefits to data warehouse applications. They
coexist with, and complement, other available indexing schemes, including standard
B-tree indexes, clustered tables, and hash clusters. While a B-tree index may be the
most efficient way to retrieve data using a unique identifier, bitmapped indexes are most
efficient when retrieving data based on much wider criteria, such as ‘How many flats were
sold last month?’ In data warehousing applications, end-users often query data based on
these wider criteria. Oracle enables efficient storage of bitmap indexes through the use of
advanced data compression technology.
Advanced join methods
Oracle offers partition-wise joins, which dramatically increase the performance of joins
involving tables that have been partitioned on the join keys. Joining records in matching
partitions increases performance, by avoiding partitions that could not possibly have
matching key records. Less memory is also used since less in-memory sorting is required.
   Hash joins deliver higher performance over other join methods in many complex
queries, especially for those queries where existing indexes cannot be leveraged in join
processing, a common occurrence in ad hoc query environments. This join eliminates the
need to perform sorts, by using an in-memory hash table constructed at runtime. The hash
join is also ideally suited for scalable parallel execution.
Sophisticated SQL optimizer
Oracle provides numerous powerful query processing techniques that are completely
transparent to the end-user. The Oracle cost-based optimizer dynamically determines
the most efficient access paths and joins for every query. It incorporates transformation
technology that automatically re-writes queries generated by end-user tools, for efficient
query execution.
   To choose the most efficient query execution strategy, the Oracle cost-based optimizer
takes into account statistics, such as the size of each table and the selectivity of each query
condition. Histograms provide the cost-based optimizer with more detailed statistics based
on a skewed, non-uniform data distribution. The cost-based optimizer optimizes execution
of queries involved in a star schema, which is common in data warehouse applications
(see Section 32.2). By using a sophisticated star-query optimization algorithm and bit-
mapped indexes, Oracle can dramatically reduce the query executions done in a traditional
join fashion. Oracle query processing not only includes a comprehensive set of specialized
techniques in all areas (optimization, access and join methods, and query execution), they
are also all seamlessly integrated, and work together to deliver the full power of the query
processing engine.
Resource management
Managing CPU and disk resources in a multi-user data warehouse or OLTP application
is challenging. As more users require access, contention for resources becomes greater.
                                             www.it-ebooks.info


1178     |   Chapter 31 z Data Warehousing Concepts
                      Oracle has resource management functionality that provides control of system resources
                      assigned to users. Important online users, such as order entry clerks, can be given a high
                      priority, while other users – those running batch reports – receive lower priorities. Users
                      are assigned to resource classes, such as ‘order entry’ or ‘batch,’ and each resource class
                      is then assigned an appropriate percentage of machine resources. In this way, high-
                      priority users are given more system resources than lower-priority users.
                      Additional data warehouse features
                      Oracle also includes many features that improve the management and performance of data
                      warehouse applications. Index rebuilds can be done online without interrupting inserts,
                      updates, or deletes that may be occurring on the base table. Function-based indexes can be
                      used to index expressions, such as arithmetic expressions, or functions that modify column
                      values. The sample scan functionality allows queries to run and only access a specified
                      percentage of the rows or blocks of a table. This is useful for getting meaningful aggregate
                      amounts, such as an average, without accessing every row of a table.
 Chapter Summary
 n Data warehousing is subject-oriented, integrated, time-variant, and non-volatile collection of data in sup-
   port of management’s decision-making process. A data warehouse is data management and data analysis
   technology.
 n Data Webhouse is a distributed data warehouse that is implemented over the Web with no central data
   repository.
 n The potential benefits of data warehousing are high returns on investment, substantial competitive advantage,
   and increased productivity of corporate decision-makers.
 n A DBMS built for Online Transaction Processing (OLTP) is generally regarded as unsuitable for data ware-
   housing because each system is designed with a differing set of requirements in mind. For example, OLTP
   systems are design to maximize the transaction processing capacity, while data warehouses are designed to
   support ad hoc query processing.
 n The major components of a data warehouse include the operational data sources, operational data store, load
   manager, warehouse manager, query manager, detailed, lightly and highly summarized data, archive/backup
   data, metadata, and end-user access tools.
 n The operational data source for the data warehouse is supplied from mainframe operational data held in first
   generation hierarchical and network databases, departmental data held in proprietary file systems, private data
   held on workstations and private servers and external systems such as the Internet, commercially available
   databases, or databases associated with an organization’s suppliers or customers.
 n The operational data store (ODS) is a repository of current and integrated operational data used for analysis.
   It is often structured and supplied with data in the same way as the data warehouse, but may in fact simply act
   as a staging area for data to be moved into the warehouse.
                                                    www.it-ebooks.info


                                                                                    Chapter Summary     |  1179
n The load manager (also called the frontend component) performs all the operations associated with the
  extraction and loading of data into the warehouse. These operations include simple transformations of the data
  to prepare the data for entry into the warehouse.
n The warehouse manager performs all the operations associated with the management of the data in the
  warehouse. The operations performed by this component include analysis of data to ensure consistency, trans-
  formation and merging of source data, creation of indexes and views, generation of denormalizations and
  aggregations, and archiving and backing-up data.
n The query manager (also called the backend component) performs all the operations associated with the
  management of user queries. The operations performed by this component include directing queries to the
  appropriate tables and scheduling the execution of queries.
n End-user access tools can be categorized into five main groups: data reporting and query tools, application
  development tools, executive information system (EIS) tools, Online Analytical Processing (OLAP) tools, and
  data mining tools.
n Data warehousing focuses on the management of five primary data flows, namely the inflow, upflow,
  downflow, outflow, and metaflow.
n Inflow is the processes associated with the extraction, cleansing, and loading of the data from the source
  systems into the data warehouse.
n Upflow is the processes associated with adding value to the data in the warehouse through summarizing,
  packaging, and distribution of the data.
n Downflow is the processes associated with archiving and backing-up of data in the warehouse.
n Outflow is the processes associated with making the data available to the end-users.
n Metaflow is the processes associated with the management of the metadata (data about data).
n The requirements for a data warehouse RDBMS include load performance, load processing, data quality
  management, query performance, terabyte scalability, mass user scalability, networked data warehouse,
  warehouse administration, integrated dimensional analysis, and advanced query functionality.
n Data mart is a subset of a data warehouse that supports the requirements of a particular department or
  business function. The issues associated with data marts include functionality, size, load performance, users’
  access to data in multiple data marts, Internet/intranet access, administration, and installation.
                                          www.it-ebooks.info


1180     |  Chapter 31 z Data Warehousing Concepts
  Review Questions
  31.1  Discuss what is meant by the following terms             (c) downflow;
        when describing the characteristics of the data          (d) outflow;
        in a data warehouse:                                     (e) metaflow.
        (a) subject-oriented;                             31.7   What are the three main approaches taken by
        (b) integrated;                                          vendors to provide data extraction, cleansing,
        (c) time-variant;                                        and transformation tools?
        (d) non-volatile.                                 31.8   Describe the specialized requirements of
  31.2  Discuss how Online Transaction Processing                a relational database management system
        (OLTP) systems differ from data warehousing              (RDBMS) suitable for use in a data
        systems.                                                 warehouse environment.
  31.3  Discuss the main benefits and problems            31.9   Discuss how parallel technologies can
        associated with data warehousing.                        support the requirements of a data
  31.4  Present a diagrammatic representation of the             warehouse.
        typical architecture and main components of       31.10  Discuss the importance of managing metadata
        a data warehouse.                                        and how this relates to the integration of the
  31.5  Describe the characteristics and main                    data warehouse.
        functions of the following components of          31.11  Discuss the main tasks associated with the
        a data warehouse:                                        administration and management of a data
        (a) load manager;                                        warehouse.
        (b) warehouse manager;                            31.12  Discuss how data marts differ from data
        (c) query manager;                                       warehouses and identify the main reasons for
        (d) metadata;                                            implementing a data mart.
        (e) end-user access tools.                        31.13  Identify the main issues associated with
  31.6  Discuss the activities associated with each of           the development and management of data
        the five primary data flows or processes within          marts.
        a data warehouse:                                 31.14  Describe the features of Oracle that
        (a) inflow;                                              support the core requirements of data
        (b) upflow;                                              warehousing.
Exercise
31.15 You are asked by the Managing Director of DreamHome to investigate and report on the applicability of data
       warehousing for the organization. The report should compare data warehouse technology with OLTP systems
       and should identify the advantages and disadvantages, and any problem areas associated with implementing
       a data warehouse. The report should reach a fully justified set of conclusions on the applicability of a data
       warehouse for DreamHome.
                                                  www.it-ebooks.info


    Chapter
32Chapter Objectives
                             Data Warehousing Design
  In this chapter you will learn:
  n  The issues associated with designing a data warehouse database.
  n  A technique for designing a data warehouse database called dimensionality
     modeling.
  n  How a dimensional model (DM) differs from an Entity–Relationship (ER) model.
  n  A step-by-step methodology for designing a data warehouse database.
  n  Criteria for assessing the degree of dimensionality provided by a data
     warehouse.
  n  How Oracle Warehouse Builder can be used to build a data warehouse.
In Chapter 31 we described the basic concepts of data warehousing. In this chapter we
focus on the issues associated with data warehouse database design. Since the 1980s, data
warehouses have evolved their own design techniques, distinct from transaction-processing
systems. Dimensional design techniques have emerged as the dominant approach for most
data warehouse databases.
                                          www.it-ebooks.info


1182  | Chapter 32 z Data Warehousing Design
                Structure of this Chapter
                In Section 32.1 we highlight the major issues associated with data warehouse design.
                In Section 32.2 we describe the basic concepts associated with dimensionality model-
                ing and then compare this technique with traditional Entity–Relationship modeling.
                In Section 32.3 we describe and demonstrate a step-by-step methodology for designing
                a data warehouse database using worked examples taken from an extended version of
                the DreamHome case study described in Section 10.4 and Appendix A. In Section 32.4
                we describe criteria for assessing the dimensionality of a data warehouse. Finally, in
                Section 32.5 we describe how to design a data warehouse using an Oracle product called
                Oracle Warehouse Builder.
     32.1       Designing a Data Warehouse Database
                Designing a data warehouse database is highly complex. To begin a data warehouse pro-
                ject, we need answers for questions such as: which user requirements are most important
                and which data should be considered first? Also, should the project be scaled down into
                something more manageable yet at the same time provide an infrastructure capable of
                ultimately delivering a full-scale enterprise-wide data warehouse? Questions such as these
                highlight some of the major issues in building data warehouses. For many enterprises the
                solution is data marts, which we described in Section 31.5. Data marts allow designers
                to build something that is far simpler and achievable for a specific group of users. Few
                designers are willing to commit to an enterprise-wide design that must meet all user
                requirements at one time. However, despite the interim solution of building data marts,
                the goal remains the same; the ultimate creation of a data warehouse that supports the
                requirements of the enterprise.
                   The requirements collection and analysis stage (see Section 9.5) of a data warehouse
                project involves interviewing appropriate members of staff such as marketing users,
                finance users, sales users, operational users, and management to enable the identification
                of a prioritized set of requirements for the enterprise that the data warehouse must meet.
                At the same time, interviews are conducted with members of staff responsible for Online
                Transaction Processing (OLTP) systems to identify, which data sources can provide clean,
                valid, and consistent data that will remain supported over the next few years.
                   The interviews provide the necessary information for the top-down view (user require-
                ments) and the bottom-up view (which data sources are available) of the data warehouse.
                With these two views defined we are ready to begin the process of designing the data ware-
                house database.
                   The database component of a data warehouse is described using a technique called dimen-
                sionality modeling. In the following sections, we first describe the concepts associated
                with a dimensional model and contrast this model with the traditional Entity–Relationship
                (ER) model (see Chapters 11 and 12). We then present a step-by-step methodology for
                creating a dimensional model using worked examples from an extended version of the
                DreamHome case study.
                                             www.it-ebooks.info


                                                                          32.2 Dimensionality Modeling | 1183
Dimensionality Modeling                                                                           32.2
  Dimensionality         A logical design technique that aims to present the data in a
  modeling               standard, intuitive form that allows for high-performance access.
Dimensionality modeling uses the concepts of Entity–Relationship (ER) modeling with
some important restrictions. Every dimensional model (DM) is composed of one table
with a composite primary key, called the fact table, and a set of smaller tables called
dimension tables. Each dimension table has a simple (non-composite) primary key that
corresponds exactly to one of the components of the composite key in the fact table. In
other words, the primary key of the fact table is made up of two or more foreign keys. This
characteristic ‘star-like’ structure is called a star schema or star join. An example star
schema for the property sales of DreamHome is shown in Figure 32.1. Note that foreign
keys (labeled {FK}) are included in a dimensional model.
   Another important feature of a DM is that all natural keys are replaced with surrogate
keys. This means that every join between fact and dimension tables is based on surrogate
keys, not natural keys. Each surrogate key should have a generalized structure based on
simple integers. The use of surrogate keys allows the data in the warehouse to have some
independence from the data used and produced by the OLTP systems. For example, each
branch has a natural key, namely branchNo and also a surrogate key namely branchID.
  Star         A logical structure that has a fact table containing factual data in the
  schema       center, surrounded by dimension tables containing reference data (which
               can be denormalized).
   The star schema exploits the characteristics of factual data such that facts are generated
by events that occurred in the past, and are unlikely to change, regardless of how they are
analyzed. As the bulk of data in a data warehouse is represented as facts, the fact tables
can be extremely large relative to the dimension tables. As such, it is important to treat
fact data as read-only reference data that will not change over time. The most useful fact
tables contain one or more numerical measures, or ‘facts’, that occur for each record. In
Figure 32.1, the facts are offerPrice, sellingPrice, saleCommission, and saleRevenue. The most
useful facts in a fact table are numeric and additive because data warehouse applications
almost never access a single record; rather, they access hundreds, thousands, or even
millions of records at a time and the most useful thing to do with so many records is to
aggregate them.
   Dimension tables, by contrast, generally contain descriptive textual information.
Dimension attributes are used as the constraints in data warehouse queries. For example,
the star schema shown in Figure 32.1 can support queries that require access to sales
of properties in Glasgow using the city attribute of the PropertyForSale table, and on sales
of properties that are flats using the type attribute in the PropertyForSale table. In fact, the
usefulness of a data warehouse is in relation to the appropriateness of the data held in the
dimension tables.
                                              www.it-ebooks.info


1184       |   Chapter 32 z Data Warehousing Design
Figure 32.1
Star schema for
property sales of
DreamHome.
                          Star schemas can be used to speed up query performance by denormalizing reference
                       information into a single dimension table. For example, in Figure 32.1 note that several
                       dimension tables (namely PropertyForSale, Branch, ClientBuyer, Staff, and Owner) contain
                       location data (city, region, and country), which is repeated in each. Denormalization is
                       appropriate when there are a number of entities related to the dimension table that are often
                       accessed, avoiding the overhead of having to join additional tables to access those
                       attributes. Denormalization is not appropriate where the additional data is not accessed
                       very often, because the overhead of scanning the expanded dimension table may not be
                       offset by any gain in the query performance.
                         Snowflake       A variant of the star schema where dimension tables do not contain
                         schema          denormalized data.
                                                    www.it-ebooks.info


                                                                          32.2 Dimensionality Modeling      |   1185
                                                                                                 Figure 32.2
                                                                                                 Part of star schema
                                                                                                 for property sales of
                                                                                                 DreamHome with a
                                                                                                 normalized version
                                                                                                 of the Branch
                                                                                                 dimension table.
   There is a variation to the star schema called the snowflake schema, which allows
dimensions to have dimensions. For example, we could normalize the location data (city,
region, and country attributes) in the Branch dimension table of Figure 32.1 to create two
new dimension tables called City and Region. A normalized version of the Branch dimen-
sion table of the property sales schema is shown in Figure 32.2. In a snowflake schema
the location data in the PropertyForSale, ClientBuyer, Staff, and Owner dimension tables would
also be removed and the new City and Region dimension tables would be shared with these
tables.
  Starflake      A hybrid structure that contains a mixture of star and snowflake
  schema         schemas.
   The most appropriate database schemas use a mixture of denormalized star and nor-
malized snowflake schemas. This combination of star and snowflake schemas is called a
starflake schema. Some dimensions may be present in both forms to cater for different
query requirements. Whether the schema is star, snowflake, or starflake, the predictable
and standard form of the underlying dimensional model offers important advantages
within a data warehouse environment including:
n  Efficiency The consistency of the underlying database structure allows more efficient
   access to the data by various tools including report writers and query tools.
n  Ability to handle changing requirements The star schema can adapt to changes in the
   user’s requirements, as all dimensions are equivalent in terms of providing access to the
   fact table. This means that the design is better able to support ad hoc user queries.
                                             www.it-ebooks.info


1186  | Chapter 32 z Data Warehousing Design
                n  Extensibility The dimensional model is extensible; for example typical changes that
                   a DM must support include: (a) adding new facts as long as they are consistent with
                   the fundamental granularity of the existing fact table; (b) adding new dimensions, as
                   long as there is a single value of that dimension defined for each existing fact record;
                   (c) adding new dimensional attributes; and (d) breaking existing dimension records
                   down to a lower level of granularity from a certain point in time forward.
                n  Ability to model common business situations There are a growing number of standard
                   approaches for handling common modeling situations in the business world. Each of
                   these situations has a well-understood set of alternatives that can be specifically pro-
                   grammed in report writers, query tools, and other user interfaces; for example, slowly
                   changing dimensions where a ‘constant’ dimension such as Branch or Staff actually
                   evolves slowly and asynchronously. We discuss slowly changing dimensions in more
                   detail in Section 32.3, Step 8.
                n  Predictable query processing Data warehouse applications that drill down will simply
                   be adding more dimension attributes from within a single star schema. Applications that
                   drill across will be linking separate fact tables together through the shared (conformed)
                   dimensions. Even though the overall suite of star schemas in the enterprise dimensional
                   model is complex, the query processing is very predictable because at the lowest level,
                   each fact table should be queried independently.
     32.2.1 Comparison of DM and ER models
                In this section we compare and contrast the dimensional model (DM) with the Entity–
                Relationship (ER) model. As described in the previous section, DMs are normally used to
                design the database component of a data warehouse whereas ER models have traditionally
                been used to describe the database for Online Transaction Processing (OLTP) systems.
                   ER modeling is a technique for identifying relationships among entities. A major
                goal of ER modeling is to remove redundancy in the data. This is immensely beneficial to
                transaction processing because transactions are made very simple and deterministic. For
                example, a transaction that updates a client’s address normally accesses a single record in
                the Client table. This access is extremely fast as it uses an index on the primary key clientNo.
                However, in making transaction processing efficient such databases cannot efficiently and
                easily support ad hoc end-user queries. Traditional business applications such as customer
                ordering, stock control, and customer invoicing require many tables with numerous joins
                between them. An ER model for an enterprise can have hundreds of logical entities, which
                can map to hundreds of physical tables. Traditional ER modeling does not support the
                main attraction of data warehousing, namely intuitive and high-performance retrieval
                of data.
                   The key to understanding the relationship between dimensional models and Entity–
                Relationship models is that a single ER model normally decomposes into multiple DMs.
                The multiple DMs are then associated through ‘shared’ dimension tables. We describe the
                relationship between ER models and DMs in more detail in the following section, in which
                we present a database design methodology for data warehouses.
                                              www.it-ebooks.info


                                            32.3 Database Design Methodology for Data Warehouses    | 1187
Database Design Methodology for                                                                  32.3
Data Warehouses
In this section we describe a step-by-step methodology for designing the database of a
data warehouse. This methodology was proposed by Kimball and is called the ‘Nine-Step
Methodology’ (Kimball, 1996). The steps of this methodology are shown in Table 32.1.
   There are many approaches that offer alternative routes to the creation of a data warehouse.
One of the more successful approaches is to decompose the design of the data warehouse
into more manageable parts, namely data marts (see Section 31.5). At a later stage, the inte-
gration of the smaller data marts leads to the creation of the enterprise-wide data warehouse.
Thus, a data warehouse is the union of a set of separate data marts implemented over a
period of time, possibly by different design teams, and possibly on different hardware and
software platforms.
   The Nine-Step Methodology specifies the steps required for the design of a data mart.
However, the methodology also ties together separate data marts so that over time they
merge together into a coherent overall data warehouse. We now describe the steps shown
in Table 32.1 in some detail using worked examples taken from an extended version of the
DreamHome case study.
Step 1: Choosing the process
The process (function) refers to the subject matter of a particular data mart. The first
data mart to be built should be the one that is most likely to be delivered on time, within
budget, and to answer the most commercially important business questions. The best
choice for the first data mart tends to be the one that is related to sales. This data source is
likely to be accessible and of high quality. In selecting the first data mart for DreamHome,
we first identify that the discrete business processes of DreamHome include:
                 Table 32.1 Nine-Step Methodology by Kimball (1996).
                  Step        Activity
                  1           Choosing the process
                  2           Choosing the grain
                  3           Identifying and conforming the dimensions
                  4           Choosing the facts
                  5           Storing pre-calculations in the fact table
                  6           Rounding out the dimension tables
                  7           Choosing the duration of the database
                  8           Tracking slowly changing dimensions
                  9           Deciding the query priorities and the query modes
                                             www.it-ebooks.info


1188     |  Chapter 32 z Data Warehousing Design
Figure 32.3  ER diagram of an extended version of DreamHome.
                     n  property sales;
                     n  property rentals (leasing);
                     n  property viewing;
                     n  property advertising;
                     n  property maintenance.
                        The data requirements associated with these processes are shown in the ER diagram of
                     Figure 32.3. Note that this ER diagram forms part of the design documentation, which
                     describes the Online Transaction Processing (OLTP) systems required to support the busi-
                     ness processes of DreamHome. The ER diagram of Figure 32.3 has been simplified by
                     labeling only the main entities and relationships and is created by following Steps 1 and 2
                     of the database design methodology described earlier in Chapters 15 and 16. The shaded
                     entities represent the core facts for each business process of DreamHome. The business
                     process selected to be the first data mart is property sales. The part of the original ER
                                                     www.it-ebooks.info


                                              32.3 Database Design Methodology for Data Warehouses              |   1189
diagram that represents the data requirements of the property sales business process is               Figure 32.4
shown in Figure 32.4.                                                                                 Part of ER diagram
                                                                                                      in Figure 32.3 that
                                                                                                      represents the data
Step 2: Choosing the grain                                                                            requirements of the
Choosing the grain means deciding exactly what a fact table record represents. For example,           property sales
                                                                                                      business process
the PropertySale entity shown with shading in Figure 32.4 represents the facts about each
                                                                                                      of DreamHome.
property sale and becomes the fact table of the property sales star schema shown
previously in Figure 32.1. Therefore, the grain of the PropertySale fact table is individual
property sales.
   Only when the grain for the fact table is chosen can we identify the dimensions of the
fact table. For example, the Branch, Staff, Owner, ClientBuyer, PropertyForSale, and Promotion
entities in Figure 32.4 will be used to reference the data about property sales and will be-
come the dimension tables of the property sales star schema shown previously in Figure 32.1.
We also include Time as a core dimension, which is always present in star schemas.
   The grain decision for the fact table also determines the grain of each of the dimension
tables. For example, if the grain for the PropertySale fact table is an individual property sale,
then the grain of the ClientBuyer dimension is the details of the client who bought a partic-
ular property.
Step 3: Identifying and conforming the dimensions
Dimensions set the context for asking questions about the facts in the fact table. A well-
built set of dimensions makes the data mart understandable and easy to use. We identify
dimensions in sufficient detail to describe things such as clients and properties at the
correct grain. For example, each client of the ClientBuyer dimension table is described by
the clientID, clientNo, clientName, clientType, city, region, and country attributes, as shown previ-
ously in Figure 32.1. A poorly presented or incomplete set of dimensions will reduce the
usefulness of a data mart to an enterprise.
                                                www.it-ebooks.info


1190       |   Chapter 32 z Data Warehousing Design
Figure 32.5               If any dimension occurs in two data marts, they must be exactly the same dimension, or
Star schemas for       one must be a mathematical subset of the other. Only in this way can two data marts share
property sales and     one or more dimensions in the same application. When a dimension is used in more than
property advertising
                       one data mart, the dimension is referred to as being conformed. Examples of dimen-
with Time,
                       sions that must conform between property sales and property advertising are the Time,
PropertyForSale,
                       PropertyForSale, Branch, and Promotion dimensions. If these dimensions are not synchronized
Branch, and
Promotion as           or if they are allowed to drift out of synchronization between data marts, the overall data
conformed (shared)     warehouse will fail, because the two data marts will not be able to be used together.
dimension tables.      For example, in Figure 32.5 we show the star schemas for property sales and property
                       advertising with Time, PropertyForSale, Branch, and Promotion as conformed dimensions with
                       light shading.
                                                    www.it-ebooks.info


                                           32.3 Database Design Methodology for Data Warehouses          |    1191
Step 4: Choosing the facts                                                                     Figure 32.6
                                                                                               Star schema for
The grain of the fact table determines which facts can be used in the data mart. All the       property rentals of
facts must be expressed at the level implied by the grain. In other words, if the grain        DreamHome. This
of the fact table is an individual property sale, then all the numerical facts must refer      is an example of a
to this particular sale. Also, the facts should be numeric and additive. In Figure 32.6 we     badly structured
use the star schema of the property rental process of DreamHome to illustrate a badly          fact table with
structured fact table. This fact table is unusable with non-numeric facts (promotionName       non-numeric facts,
and staffName), a non-additive fact (monthlyRent), and a fact (lastYearRevenue) at a different a non-additive fact,
granularity from the other facts in the table. Figure 32.7 shows how the Lease fact            and a numeric fact
                                                                                               with an inconsistent
table shown in Figure 32.6 could be corrected so that the fact table is appropriately
                                                                                               granularity with the
structured.
                                                                                               other facts in the
   Additional facts can be added to a fact table at any time provided they are consistent      table.
with the grain of the table.
                                            www.it-ebooks.info


1192       |   Chapter 32 z Data Warehousing Design
Figure 32.7
Star schema for the
property rentals of
DreamHome. This is
the schema shown in
Figure 32.6 with the
problems corrected.
                       Step 5: Storing pre-calculations in the fact table
                       Once the facts have been selected each should be re-examined to determine whether there
                       are opportunities to use pre-calculations. A common example of the need to store pre-
                       calculations occurs when the facts comprise a profit and loss statement. This situation will
                       often arise when the fact table is based on invoices or sales. Figure 32.7 shows the fact table
                       with the rentDuration, totalRent, clientAllowance, staffCommission, and totalRevenue attributes. These
                       types of facts are useful because they are additive quantities, from which we can derive
                       valuable information such as the average clientAllowance based on aggregating some number
                       of fact table records. To calculate the totalRevenue generated per property rental we subtract
                       the clientAllowance and the staffCommission from totalRent. Although the totalRevenue can always
                       be derived from these attributes, we still need to store the totalRevenue. This is particularly
                       true for a value that is fundamental to an enterprise, such as totalRevenue, or if there is any
                       chance of a user calculating the totalRevenue incorrectly. The cost of a user incorrectly rep-
                       resenting the totalRevenue is offset against the minor cost of a little redundant data storage.
                                                       www.it-ebooks.info


                                          32.3 Database Design Methodology for Data Warehouses | 1193
Step 6: Rounding out the dimension tables
In this step, we return to the dimension tables and add as many text descriptions to the
dimensions as possible. The text descriptions should be as intuitive and understandable to
the users as possible. The usefulness of a data mart is determined by the scope and nature
of the attributes of the dimension tables.
Step 7: Choosing the duration of the database
The duration measures how far back in time the fact table goes. In many enterprises,
there is a requirement to look at the same time period a year or two earlier. For other enter-
prises, such as insurance companies, there may be a legal requirement to retain data
extending back five or more years. Very large fact tables raise at least two very significant
data warehouse design issues. First, it is often increasingly difficult to source increasingly
old data. The older the data, the more likely there will be problems in reading and
interpreting the old files or the old tapes. Second, it is mandatory that the old versions
of the important dimensions be used, not the most current versions. This is known as the
‘slowly changing dimension’ problem, which is described in more detail in the follow-
ing step.
Step 8: Tracking slowly changing dimensions
The slowly changing dimension problem means, for example, that the proper description
of the old client and the old branch must be used with the old transaction history. Often,
the data warehouse must assign a generalized key to these important dimensions in order
to distinguish multiple snapshots of clients and branches over a period of time.
   There are three basic types of slowly changing dimensions: Type 1, where a changed
dimension attribute is overwritten; Type 2, where a changed dimension attribute causes a
new dimension record to be created; and Type 3, where a changed dimension attribute
causes an alternate attribute to be created so that both the old and new values of the attri-
bute are simultaneously accessible in the same dimension record.
Step 9: Deciding the query priorities and the query modes
In this step we consider physical design issues. The most critical physical design issues
affecting the end-user’s perception of the data mart are the physical sort order of the fact
table on disk and the presence of pre-stored summaries or aggregations. Beyond these issues
there are a host of additional physical design issues affecting administration, backup,
indexing performance, and security. For further information on the issues affecting the
physical design for data warehouses the interested reader is referred to Anahory and
Murray (1997).
At the end of this methodology, we have a design for a data mart that supports the
requirements of a particular business process and also allows the easy integration with
other related data marts to ultimately form the enterprise-wide data warehouse. Table 32.2
lists the fact and dimension tables associated with the star schema for each business process
of DreamHome (identified in Step 1 of the methodology).
                                            www.it-ebooks.info


1194       |    Chapter 32 z Data Warehousing Design
Figure 32.8                We integrate the star schemas for the business processes of DreamHome using the con-
Dimensional model       formed dimensions. For example, all the fact tables share the Time and Branch dimensions
(fact constellation)    as shown in Table 32.2. A dimensional model, which contains more than one fact table
for the DreamHome       sharing one or more conformed dimension tables, is referred to as a fact constellation.
data warehouse.
                        The fact constellation for the DreamHome data warehouse is shown in Figure 32.8. The
                        model has been simplified by displaying only the names of the fact and dimension tables.
                        Note that the fact tables are shown with dark shading and all the dimension tables being
                        conformed are shown with light shading.
                                                     www.it-ebooks.info


                                 32.4 Criteria for Assessing the Dimensionality of a Data Warehouse | 1195
Table 32.2 Fact and dimension tables for each business process of DreamHome.
  Business process        Fact table               Dimension tables
  Property sales          PropertySale             Time, Branch, Staff, PropertyForSale, Owner,
                                                   ClientBuyer, Promotion
  Property rentals        Lease                    Time, Branch, Staff, PropertyForRent, Owner,
                                                   ClientRenter, Promotion
  Property viewing        PropertyViewing          Time, Branch, PropertyForSale,
                                                   PropertyForRent, ClientBuyer, ClientRenter
  Property advertising    Advert                   Time, Branch, PropertyForSale,
                                                   PropertyForRent, Promotion, Newspaper
  Property maintenance    PropertyMaintenance      Time, Branch, Staff, PropertyForRent
Criteria for Assessing the Dimensionality                                                       32.4
of a Data Warehouse
Since the 1980s, data warehouses have evolved their own design techniques, distinct from
OLTP systems. Dimensional design techniques have emerged as the main approach for
most of the data warehouses. In this section we describe the criteria proposed by Ralph
Kimball to measure the extent to which a system supports the dimensional view of data
warehousing (Kimball, 2000a,b).
   When assessing a particular data warehouse remember that few vendors attempt to
provide a completely integrated solution. However, as a data warehouse is a complete
system, the criteria should only be used to assess complete end-to-end systems and not a
collection of disjointed packages that may never integrate well together.
   There are twenty criteria divided into three broad groups: architecture, administration,
and expression as shown in Table 32.3. The purpose of establishing these criteria is to
establish an objective standard for assessing how well a system supports the dimensional
view of data warehousing, and to set the threshold high so that vendors have a target for
improving their systems. The intended way to use this list is to rate a system on each
criterion with a simple 0 or 1. A system qualifies for a 1 only if it meets the full definition
of support for that criterion. For example, a system that offers aggregate navigation
(the fourth criterion) that is available only to a single front-end tool gets a zero because
the aggregate navigation is not open. In other words, there can be no partial credit for a
criterion.
   Architectural criteria are fundamental characteristics to the way the entire system is
organized. These criteria usually extend from the backend, through the DBMS, to the
frontend and the user’s desktop.
   Administration criteria are more tactical than architectural criteria, but are considered
to be essential to the ‘smooth running’ of a dimensionally oriented data warehouse.
These criteria generally affect IT personnel who are building and maintaining the data
warehouse.
                                             www.it-ebooks.info


1196  | Chapter 32 z Data Warehousing Design
                            Table 32.3 Criteria for assessing the dimensionality provided by a data
                            warehouse (Kimball, 2000a,b).
                              Group                      Criteria
                              Architecture               Explicit declaration
                                                         Conformed dimensions and facts
                                                         Dimensional integrity
                                                         Open aggregate navigation
                                                         Dimensional symmetry
                                                         Dimensional scalability
                                                         Sparsity tolerance
                              Administration             Graceful modification
                                                         Dimensional replication
                                                         Changed dimension notification
                                                         Surrogate key administration
                                                         International consistency
                              Expression                 Multiple-dimension hierarchies
                                                         Ragged-dimension hierarchies
                                                         Multiple valued dimensions
                                                         Slowly changing dimensions
                                                         Roles of a dimension
                                                         Hot-swappable dimensions
                                                         On-the-fly fact range dimensions
                                                         On-the-fly behavior dimensions
                   Expression criteria are mostly analytic capabilities that are needed in real-life situ-
                ations. The end-user community experiences all expression criteria directly. The expression
                criteria for dimensional systems are not the only features users look for in a data ware-
                house, but they are all capabilities that need to exploit the power of a dimensional system.
                   A system that supports most or all of these dimensional criteria would be adaptable,
                easier to administer, and able to address many real-world applications. The major point of
                dimensional systems is that they are business-issue and end-user driven. For further details
                of the criteria in Table 32.3, the interested reader is referred to Kimball (2000a,b).
     32.5       Data Warehousing Design Using Oracle
                We introduced the Oracle DBMS in Section 8.2. In this section, we describe Oracle
                Warehouse Builder (OWB) as a key component of the Oracle Warehouse solution,
                enabling the design and deployment of data warehouses, data marts, and e-Business intelli-
                gence applications. OWB is a design tool and an extraction, transformation, and loading
                                             www.it-ebooks.info


                                                      32.5 Data Warehousing Design Using Oracle     | 1197
(ETL) tool. An important aspect of OWB from the customers’ perspective is that it allows
the integration of the traditional data warehousing environments with the new e-Business
environments (Oracle Corporation, 2000). This section first provides an overview of the
components of OWB and the underlying technologies and then describes how the user
would apply OWB to typical data warehousing tasks.
Oracle Warehouse Builder Components                                                            32.5.1
OWB provides the following primary functional components:
n  A repository consisting of a set of tables in an Oracle database that is accessed via a
   Java-based access layer. The repository is based on the Common Warehouse Model
   (CWM) standard, which allows the OWB meta-data to be accessible to other products
   that support this standard (see Section 31.4.3).
n  A graphical user interface (GUI) that enables access to the repository. The GUI
   features graphical editors and an extensive use of wizards. The GUI is written in Java,
   making the frontend portable.
n  A code generator, also written in Java, generates the code that enables the deployment
   of data warehouses. The different code types generated by OWB are discussed later in
   this section.
n  Integrators, which are components that are dedicated to extracting data from a particular
   type of source. In addition to native support for Oracle, other relational, non-relational,
   and flat-file data sources, OWB integrators allow access to information in enterprise
   resource planning (ERP) applications such as Oracle and SAP R/3. The SAP integrator
   provides access to SAP transparent tables using PL/SQL code generated by OWB.
n  An open interface that allows developers to extend the extraction capabilities of OWB,
   while leveraging the benefits of the OWB framework. This open interface is made avail-
   able to developers as part of the OWB Software Development Kit (SDK).
n  Runtime, which is a set of tables, sequences, packages, and triggers that are installed
   in the target schema. These database objects are the foundation for the auditing and
   error detection/correction capabilities of OWB. For example, loads can be restarted
   based on information stored in the runtime tables. OWB includes a runtime audit viewer
   for browsing the runtime tables and runtime reports.
The architecture of the Oracle Warehouse Builder is shown in Figure 32.9. Oracle Ware-
house Builder is a key component of the larger Oracle data warehouse. The other products
that the OWB must work with within the data warehouse include:
n  Oracle – the engine of OWB (as there is no external server);
n  Oracle Enterprise Manager – for scheduling;
n  Oracle Workflow – for dependency management;
n  Oracle Pure•Extract – for MVS mainframe access;
n  Oracle Pure•Integrate – for customer data quality;
n  Oracle Gateways – for relational and mainframe data access.
                                           www.it-ebooks.info


1198       |   Chapter 32 z Data Warehousing Design
Figure 32.9
Oracle Warehouse
Builder architecture.
         32.5.2 Using Oracle Warehouse Builder
                       In this section we describe how OWB assists the user in some typical data warehousing
                       tasks like defining source data structures, designing the target warehouse, mapping sources
                       to targets, generating code, instantiating the warehouse, extracting the data, and maintain-
                       ing the warehouse.
                       Defining sources
                       Once the requirements have been determined and all the data sources have been identified,
                       a tool such as OWB can be used for constructing the data warehouse. OWB can handle
                       a diverse set of data sources by means of integrators. OWB also has the concept of a
                       module, which is a logical grouping of related objects. There are two types of modules:
                       data source and warehouse. For example, a data source module might contain all the
                       definitions of the tables in an OLTP database that is a source for the data warehouse.
                       And a module of type warehouse might contain definitions of the facts, dimensions, and
                       staging tables that make up the data warehouse. It is important to note that modules merely
                       contain definitions, that is metadata, about either sources or warehouses, and not objects
                       that can be populated or queried. A user identifies the integrators that are appropriate
                       for the data sources, and each integrator accesses a source and imports the metadata
                       that describes it.
                       Oracle sources
                       To connect to an Oracle database, the user chooses the integrator for Oracle databases.
                       Next, the user supplies some more detailed connection information, for example user
                       name, password, and SQL*Net connection string. This information is used to define a
                       database link in the database that hosts the OWB repository. OWB uses this database link
                       to query the system catalog of the source database and extract metadata that describes the
                       tables and views of interest to the user. The user experiences this as a process of visually
                       inspecting the source and selecting objects of interest.
                                                    www.it-ebooks.info


                                                         32.5 Data Warehousing Design Using Oracle | 1199
Non-Oracle sources
Non-Oracle databases are accessed in exactly the same way as Oracle databases. What
makes this possible is the Transparent Gateway technology of Oracle. In essence, a
Transparent Gateway allows a non-Oracle database to be treated in exactly the same
way as if it were an Oracle database. On the SQL level, once the database link pointing to
the non-Oracle database has been defined, the non-Oracle database can be queried via
SELECT just like any Oracle database. In OWB, all the user has to do is identify the type
of database, so that OWB can select the appropriate Transparent Gateway for the database
link definition. In the case of MVS mainframe sources, OWB and Oracle Pure•Extract
provide data extraction from sources such as IMS, DB2, and VSAM. The plan is that
Oracle Pure•Extract will ultimately be integrated with the OWB technology.
Flat files
OWB supports two kinds of flat files: character-delimited and fixed-length files. If the data
source is a flat file, the user selects the integrator for flat files and specifies the path and
file name. The process of creating the meta-data that describes a file is different from the
process used for a table in a database. With a table, the owning database itself stores
extensive information about the table such as the table name, the column names, and data
types. This information can be easily queried from the catalog. With a file, on the other
hand, the user assists in the process of creating the metadata with some intelligent guesses
supplied by OWB. In OWB, this process is called sampling.
Web data
With the proliferation of the Internet, the new challenge for data warehousing is to capture
data from Web sites. There are different types of data in e-Business environments: trans-
actional Web data stored in the underlying databases; clickstream data stored in Web server
log files; registration data in databases or log files; and consolidated clickstream data in
the log files of Web analysis tools. OWB can address all these sources with its built-in
features for accessing databases and flat files.
Data quality
A solution to the challenge of data quality is OWB with Oracle Pure•Integrate. Oracle
Pure•Integrate is customer data integration software that automates the creation of con-
solidated profiles of customers and related business data to support e-Business and
customer relationship management applications. Pure•Integrate complements OWB by
providing advanced data transformation and cleansing features designed specifically to
meet the requirements of database applications. These include:
n  integrated name and address processing to standardize, correct, and enhance representa-
   tions of customer names and locations;
n  advanced probabilistic matching to identify unique consumers, businesses, households,
   super-households, or other entities for which no common identifiers exist;
n  powerful rule-based merging to resolve conflicting data and create the ‘best possible’
   integrated result from the matched data.
                                             www.it-ebooks.info


1200 | Chapter 32 z Data Warehousing Design
               Designing the target warehouse
               Once the source systems have been identified and defined, the next task is to design the
               target warehouse based on user requirements. One of the most popular designs in data
               warehousing is the star schema and its variations, as discussed in Section 32.2. Also, many
               business intelligence tools such as Oracle Discoverer are optimized for this kind of design.
               OWB supports all variations of star schema designs. It features wizards and graphical
               editors for fact and dimensions tables. For example, in the Dimension Editor the user
               graphically defines the attributes, levels, and hierarchies of a dimension.
               Mapping sources to targets
               When both the sources and the target have been well defined, the next step is to map the
               two together. Remember that there are two types of modules: source modules and ware-
               house modules. Modules can be reused many times in different mappings. Warehouse
               modules can themselves be used as source modules. For example, in an architecture where
               we have an OLTP database that feeds a central data warehouse, which in turn feeds a data
               mart, the data warehouse is a target (from the perspective of the OLTP database) and a
               source (from the perspective of the data mart).
                  The mappings of OWB are defined on two levels. A high-level mapping that indicates
               source and target modules. One level down is the detail mapping that allows a user to map
               source columns to target columns and defines transformations. OWB features a built-in
               transformation library from which the user can pick predefined transformations. Users can
               also define their own transformations in PL/SQL and Java.
               Generating code
               The Code Generator is the OWB component that reads the target definitions and source-
               to-target mappings and generates code to implement the warehouse. The type of generated
               code varies depending on the type of object that the user wants to implement.
               Logical versus physical design
               Before generating code, the user has primarily been working on the logical level, that is,
               on the level of object definitions. On this level, the user is concerned with capturing all the
               details and relationships (the semantics) of an object, but is not yet concerned with
               defining any implementation characteristics. For example, consider a table to be imple-
               mented in an Oracle database. On the logical level, the user may be concerned with the
               table name, the number of columns, the column names and data types, and any relation-
               ships that the table has to other tables. On the physical level, however, the question
               becomes: how can this table be optimally implemented in an Oracle database? The user
               must now be concerned with things like tablespaces, indexes, and storage parameters (see
               Section 8.2.2). OWB allows the user to view and manipulate an object on both the logical
               and physical level. The logical definition and physical implementation details are auto-
               matically synchronized.
                                             www.it-ebooks.info


                                                      32.5 Data Warehousing Design Using Oracle | 1201
Configuration
In OWB, the process of assigning physical characteristics to an object is called configura-
tion. The specific characteristics that can be defined depend on the object that is being
configured. These objects include, for example, storage parameters, indexes, tablespaces,
and partitions.
Validation
It is good practice to check the object definitions for completeness and consistency prior
to code generation. OWB offers a validate feature to automate this process. Errors detect-
able by the validation process include, for example, data type mismatches between sources
and targets, and foreign key errors.
Generation
The following are some of the main types of code that OWB produces:
n  SQL Data Definition Language (DDL) commands A warehouse module with its
   definitions of fact and dimension tables is implemented as a relational schema in an
   Oracle database. OWB generates SQL DDL scripts that create this schema. The scripts
   can either be executed from within OWB or saved to the file system for later, manual
   execution.
n  PL/SQL programs A source-to-target mapping results in a PL/SQL program if the
   source is a database, whether Oracle or non-Oracle. The PL/SQL program accesses
   the source database via a database link, performs the transformations as defined in the
   mapping, and loads the data into the target table.
n  SQL*Loader control files If the source in a mapping is a flat file, OWB generates a
   control file for use with SQL*Loader.
n  Tcl scripts OWB also generates Tcl scripts. These can be used to schedule PL/SQL
   and SQL*Loader mappings as jobs in Oracle Enterprise Manager – for example, to
   refresh the warehouse at regular intervals.
Instantiating the warehouse and extracting data
Before the data can be moved from the source to the target database, the developer has to
instantiate the warehouse, in other words execute the generated DDL scripts to create the
target schema. OWB refers to this step as deployment. Once the target schema is in place,
the PL/SQL programs can move data from the source into the target. Note that the basic
data movement mechanism is INSERT . . . SELECT . . . with the use of a database link.
If an error should occur, a routine from one of the OWB runtime packages logs the error
in an audit table.
Maintaining the warehouse
Once the data warehouse has been instantiated and the initial load has been completed, it
has to be maintained. For example, the fact table has to be refreshed at regular intervals,
so that queries return up-to-date results. Dimension tables have to be extended and
                                           www.it-ebooks.info


1202    |   Chapter 32 z Data Warehousing Design
                     updated, albeit much less frequently than fact tables. An example of a slowly changing
                     dimension is the Customer table, in which a customer’s address, marital status, or name
                     may all change over time. In addition to INSERT, OWB also supports other ways of
                     manipulating the warehouse:
                     n  UPDATE
                     n  DELETE
                     n  INSERT/UPDATE (insert a row; if it already exists, update it)
                     n  UPDATE/INSERT (update a row; if it does not exist, insert it)
                     These features give the OWB user a variety of tools to undertake ongoing maintenance
                     tasks. OWB interfaces with Oracle Enterprise Manager for repetitive maintenance tasks;
                     for example, a fact table refresh that is scheduled to occur at a regular interval. For com-
                     plex dependencies OWB integrates with Oracle Workflow.
                     Metadata integration
                     OWB is based on the Common Warehouse Model (CWM) standard (see Section 31.4.3).
                     It can seamlessly exchange metadata with Oracle Express and Oracle Discoverer as well
                     as other business intelligence tools that comply with the standard.
 Chapter Summary
 n Dimensionality modeling is a design technique that aims to present the data in a standard, intuitive form that
   allows for high-performance access.
 n Every dimensional model (DM) is composed of one table with a composite primary key, called the fact table,
   and a set of smaller tables called dimension tables. Each dimension table has a simple (non-composite)
   primary key that corresponds exactly to one of the components of the composite key in the fact table. In other
   words, the primary key of the fact table is made up of two or more foreign keys. This characteristic ‘star-like’
   structure is called a star schema or star join.
 n Star schema is a logical structure that has a fact table containing factual data in the center, surrounded by
   dimension tables containing reference data (which can be denormalized).
 n The star schema exploits the characteristics of factual data such that facts are generated by events that
   occurred in the past, and are unlikely to change, regardless of how they are analyzed. As the bulk of data in
   the data warehouse is represented within facts, the fact tables can be extremely large relative to the dimension
   tables.
 n The most useful facts in a fact table are numerical and additive because data warehouse applications almost
   never access a single record; rather, they access hundreds, thousands, or even millions of records at a time and
   the most useful thing to do with so many records is to aggregate them.
 n Dimension tables most often contain descriptive textual information. Dimension attributes are used as the
   constraints in data warehouse queries.
 n Snowflake schema is a variant of the star schema where dimension tables do not contain denormalized data.
 n Starflake schema is a hybrid structure that contains a mixture of star and snowflake schemas.
                                                   www.it-ebooks.info


                                                                                               Exercises    |   1203
  n  The key to understanding the relationship between dimensional models and ER models is that a single ER
     model normally decomposes into multiple DMs. The multiple DMs are then associated through conformed
     (shared) dimension tables.
  n  There are many approaches that offer alternative routes to the creation of a data warehouse. One of the more
     successful approaches is to decompose the design of the data warehouse into more manageable parts, namely
     data marts. At a later stage, the integration of the smaller data marts leads to the creation of the enterprise-
     wide data warehouse.
  n  The Nine-Step Methodology specifies the steps required for the design of a data mart / warehouse. The steps
    include: Step 1 Choosing the process, Step 2 Choosing the grain, Step 3 Identifying and conforming the
    dimensions, Step 4 Choosing the facts, Step 5 Storing pre-calculations in the fact table, Step 6 Rounding out
    the dimensions, Step 7 Choosing the duration of the database, Step 8 Tracking slowly changing dimensions,
    and Step 9 Deciding the query priorities and query modes.
  n  There are criteria to measure the extent to which a system supports the dimensional view of data warehous-
     ing. The criteria are divided into three broad groups: architecture, administration, and expression.
  n  Oracle Warehouse Builder (OWB) is a key component of the Oracle Warehouse solution, enabling the
     design and deployment of data warehouses, data marts, and e-Business intelligence applications. OWB is both
     a design tool and an extraction, transformation, and loading (ETL) tool.
  Review Questions
  31.1    Identify the major issues associated with                 warehouse environment. Describe these
          designing a data warehouse database.                      advantages.
  31.2    Describe how a dimensional model (DM)              31.7 Describe the main activities associated with
          differs from an Entity–Relationship (ER)                  each step of the Nine-Step Methodology for
          model.                                                    data warehouse database design.
  31.3    Present a diagrammatic representation of a         31.8 Describe the purpose of assessing the
          typical star schema.                                      dimensionality of a data warehouse.
  31.4    Describe how the fact and dimensional tables       31.9 Briefly outline the criteria groups used to
          of a star schema differ.                                  assess the dimensionality of a data
  31.5    Describe how star, snowflake, and starflake               warehouse.
          schemas differ.                                    31.10 Describe how the Oracle Warehouse
  31.6    The star, snowflake, and starflake schemas                Builder supports the design of a data
          offer important advantages in a data                      warehouse.
Exercises
31.11 Use the Nine-Step Methodology for data warehouse database design to produce dimensional models for the
        case studies described in Appendix B.
31.12 Use the Nine-Step Methodology for data warehouse database design to produce a dimensional model for all
        or part of your organization.
                                              www.it-ebooks.info


    Chapter
33Chapter Objectives
                             OLAP
  In this chapter you will learn:
  n  The purpose of Online Analytical Processing (OLAP).
  n  The relationship between OLAP and data warehousing.
  n  The key features of OLAP applications.
  n  The potential benefits associated with successful OLAP applications.
  n  How to represent multi-dimensional data.
  n  The rules for OLAP tools.
  n  The main categories of OLAP tools.
  n  OLAP extensions to the SQL standard.
  n  How Oracle supports OLAP.
In Chapter 31 we discussed the increasing popularity of data warehousing as a means of
gaining competitive advantage. We learnt that data warehouses bring together large
volumes of data for the purposes of data analysis. Until recently, access tools for large
database systems have provided only limited and relatively simplistic data analysis.
However, accompanying the growth in data warehousing is an ever-increasing demand by
users for more powerful access tools that provide advanced analytical capabilities. There
are two main types of access tools available to meet this demand, namely Online
Analytical Processing (OLAP) and data mining. These tools differ in what they offer the
user and because of this they are complementary technologies.
   A data warehouse (or more commonly one or more data marts) together with tools such
as OLAP and/or data mining are collectively referred to as Business Intelligence (BI)
technologies. In this chapter we describe OLAP and in the following chapter we describe
data mining.
                             www.it-ebooks.info


                                                                    33.1 Online Analytical Processing | 1205
  Structure of this Chapter
In Section 33.1 we introduce Online Analytical Processing (OLAP) and discuss the
relationship between OLAP and data warehousing. In Section 33.2 we describe OLAP
applications and identify the key features and potential benefits associated with OLAP
applications. In Section 33.3 we discuss how multi-dimensional data can be represented
and describe the main concepts associated with multi-dimensional analysis. In Section
33.4 we describe the rules for OLAP tools and highlight the characteristics and issues
associated with OLAP tools. In Section 33.5 we discuss how the SQL standard has been
extended to include OLAP functions. Finally, in Section 33.6, we describe how Oracle
supports OLAP. The examples in this chapter are taken from the DreamHome case study
described in Section 10.4 and Appendix A.
Online Analytical Processing                                                                     33.1
Over the past few decades, we have witnessed the increasing popularity and prevalence of
relational DBMSs such that we now find a significant proportion of corporate data is housed
in such systems. Relational databases have been used primarily to support traditional
Online Transaction Processing (OLTP) systems. To provide appropriate support for OLTP
systems, relational DBMSs have been developed to enable the highly efficient execution
of a large number of relatively simple transactions.
   In the past few years, relational DBMS vendors have targeted the data warehousing
market and have promoted their systems as tools for building data warehouses. As dis-
cussed in Chapter 31, a data warehouse stores operational data and is expected to support
a wide range of queries from the relatively simple to the highly complex. However, the
ability to answer particular queries is dependent on the types of end-user access tools
available for use on the data warehouse. General-purpose tools such as reporting and query
tools can easily support ‘who?’ and ‘what?’ questions about past events. A typical query
submitted directly to a data warehouse is: ‘What was the total revenue for Scotland in the
third quarter of 2004?’. In this section we focus on a tool that can support more advanced
queries, namely Online Analytical Processing (OLAP).
  Online Analytical          The dynamic synthesis, analysis, and consolidation of large
  Processing (OLAP)          volumes of multi-dimensional data.
   OLAP is a term that describes a technology that uses a multi-dimensional view of aggre-
gate data to provide quick access to strategic information for the purposes of advanced
analysis (Codd et al., 1995). OLAP enables users to gain a deeper understanding and know-
ledge about various aspects of their corporate data through fast, consistent, interactive access
to a wide variety of possible views of the data. OLAP allows the user to view corporate
data in such a way that it is a better model of the true dimensionality of the enterprise.
While OLAP systems can easily answer ‘who?’ and ‘what?’ questions, it is their ability to
answer ‘what if?’ and ‘why?’ type questions that distinguishes them from general-purpose
                                           www.it-ebooks.info


1206  | Chapter 33 z OLAP
                query tools. OLAP enables decision-making about future actions. A typical OLAP calcula-
                tion can be more complex than simply aggregating data, for example, ‘Compare the num-
                bers of properties sold for each type of property in the different regions of Great Britain
                for each year since 2000.’ Hence, the types of analysis available from OLAP range from
                basic navigation and browsing (referred to as ‘slicing and dicing’), to calculations, to more
                complex analyses such as time series and complex modeling.
     33.1.1 OLAP Benchmarks
                The OLAP Council has published an analytical processing benchmark referred to as the
                APB-1 (OLAP Council, 1998). The aim of the APB-1 is to measure a server’s overall
                OLAP performance rather than the performance of individual tasks. To ensure the
                relevance of the APB-1 to actual business applications, the operations performed on the
                database are based on the most common business operations, which include the following:
                n  bulk loading of data from internal or external data sources;
                n  incremental loading of data from operational systems;
                n  aggregation of input-level data along hierarchies;
                n  calculation of new data based on business models;
                n  time series analysis;
                n  queries with a high degree of complexity;
                n  drill-down through hierarchies;
                n  ad hoc queries;
                n  multiple online sessions.
                OLAP applications are also judged on their ability to provide just-in-time (JIT) information,
                which is regarded as being a core requirement of supporting effective decision-making.
                Assessing a server’s ability to satisfy this requirement is more than measuring processing
                performance and includes its abilities to model complex business relationships and to
                respond to changing business requirements.
                   To allow for comparison of performances of different combinations of hardware and
                software, a standard benchmark metric called Analytical Queries per Minute (AQM) has
                been defined. The AQM represents the number of analytical queries processed per minute
                including data loading and computation time. Thus, the AQM incorporates data loading
                performance, calculation performance, and query performance into a singe metric.
                   Publication of APB-1 benchmark results must include both the database schema and all
                code required for executing the benchmark. This allows the evaluation of a given solution
                in terms of both its quantitative and qualitative appropriateness to the task.
     33.2       OLAP Applications
                There are many examples of OLAP applications in various functional areas as listed in
                Table 33.1 (OLAP Council, 2001).
                                             www.it-ebooks.info


                                                                                 33.2 OLAP Applications | 1207
Table 33.1   Examples of OLAP applications in various functional areas.
  Functional area             Examples of OLAP applications
  Finance                     Budgeting, activity-based costing, financial performance analysis,
                              and financial modeling
  Sales                       Sales analysis and sales forecasting
  Marketing                   Market research analysis, sales forecasting, promotions analysis,
                              customer analysis, and market/customer segmentation
  Manufacturing               Production planning and defect analysis
   An essential requirement of all OLAP applications is the ability to provide users with
JIT information, which is necessary to make effective decisions about an organization’s
strategic directions. JIT information is computed data that usually reflects complex rela-
tionships and is often calculated on-the-fly. Analysing and modeling complex relationships
are practical only if response times are consistently short. In addition, because the nature
of data relationships may not be known in advance, the data model must be flexible.
A truly flexible data model ensures that OLAP systems can respond to changing business
requirements as required for effective decision-making. Although OLAP applications are
found in widely divergent functional areas, they all require the following key features as
described in the OLAP Council White Paper (2001):
n  multi-dimensional views of data;
n  support for complex calculations;
n  time intelligence.
Multi-dimensional views of data
The ability to represent multi-dimensional views of corporate data is a core requirement
of building a ‘realistic’ business model. For example, in the case of DreamHome users
may require to view property sales data by property type, property location, branch, sales
personnel, and time. A multi-dimensional view of data provides the basis for analytical
processing through flexible access to corporate data. Furthermore, the underlying database
design that provides the multi-dimensional view of data should treat all dimensions
equally. In other words, the database design should:
n  not influence the types of operations that are allowable on a given dimension or the rate
   at which these operations are performed;
n  enable users to analyze data across any dimension at any level of aggregation with equal
   functionality and ease;
n  support all multi-dimensional views of data in the most intuitive way possible.
OLAP systems should as much as possible hide users from the syntax of complex queries
and provide consistent response times for all queries no matter how complex. The
                                            www.it-ebooks.info


1208  | Chapter 33 z OLAP
                OLAP Council APB-1 performance benchmark tests a server’s ability to provide a
                multi-dimensional view of data by requiring queries of varying complexity and scope. A
                consistently quick response time for these queries is a key measure of a server’s ability to
                meet this requirement.
                Support for complex calculations
                OLAP software must provide a range of powerful computational methods such as that
                required by sales forecasting, which uses trend algorithms such as moving averages
                and percentage growth. Furthermore, the mechanisms for implementing computational
                methods should be clear and non-procedural. This should enable users of OLAP to
                work in a more efficient and self-sufficient way. The OLAP Council APB-1 performance
                benchmark contains a representative selection of calculations, both simple (such as the
                calculation of budgets) and complex (such as forecasting).
                Time intelligence
                Time intelligence is a key feature of almost any analytical application as performance is
                almost always judged over time, for example, this month versus last month or this month
                versus the same month last year. The time hierarchy is not always used in the same manner
                as other hierarchies. For example, a user may require to view, the sales for the month of
                May or the sales for the first five months of 2004. Concepts such as year-to-date and
                period-over-period comparisons should be easily defined in an OLAP system. The OLAP
                Council APB-1 performance benchmark contains examples of how time is used in OLAP
                applications such as computing a three-month moving average or forecasting, which uses
                this year’s versus last year’s data.
     33.2.1 OLAP Benefits
                The benefits that potentially follow the successful implementation of an OLAP application
                include:
                n  Increased productivity of business end-users, IT developers, and consequently the entire
                   organization. More controlled and timely access to strategic information can allow more
                   effective decision-making.
                n  Reduced backlog of applications development for IT staff by making end-users self-
                   sufficient enough to make their own schema changes and build their own models.
                n  Retention of organizational control over the integrity of corporate data as OLAP
                   applications are dependent on data warehouses and OLTP systems to refresh their
                   source level data.
                n  Reduced query drag and network traffic on OLTP systems or on the data warehouse.
                n  Improved potential revenue and profitability by enabling the organization to respond
                   more quickly to market demands.
                                             www.it-ebooks.info


                                                       33.3 Representation of Multi-Dimensional Data | 1209
Representation of Multi-Dimensional Data                                                         33.3
In this section we consider the alternative ways of representing multi-dimensional data.
For example, how should we best represent the query, ‘What is the total revenue gener-
ated by property sales in each city, in each quarter of 2004?’. This revenue data can fit
into a three-field relational table, as shown in Figure 33.1(a), however, this data fits
much more naturally into a two-dimensional matrix, with the dimensions being City and
Time (quarters), as shown in Figure 33.1(b). What differentiates the requirements for
these representations are the queries that the end-user may ask. If the user simply poses
queries like ‘What was the revenue for Glasgow in the first quarter?’ and other queries
that retrieve only a single value, then there would be no need to structure this data in a
multi-dimensional database. However, if the user asks questions like ‘What is the total
annual revenue for each city?’ or ‘What is the average revenue for each city?’, then this
involves retrieving multiple values and aggregating them. If we consider large databases
consisting of thousands of cities, the time that it takes a relational DBMS to perform
these types of calculation becomes significant. A typical RDBMS can scan a few hundred
records per second. A typical multi-dimensional DBMS can perform aggregations at a rate
of 10,000 per second or more.
   Consider the revenue data with an additional dimension, namely property type. In this
case, the data represents the total revenue generated by the sale of each type of property
(for simplicity, we use only Flat and House), by city, and by time (quarters). Again, this
data can fit into a four-field table, as shown in Figure 33.1(c), however, the data fits more
naturally into a three-dimensional cube, as shown in Figure 33.1(d). The cube represents
data as cells in an array by associating the total revenue with the dimensions Property Type,
City, and Time. The table in an RDBMS can only ever represent multi-dimensional data in
two dimensions.
   OLAP database servers use multi-dimensional structures to store data and relationships
between data. Multi-dimensional structures are best visualized as cubes of data, and cubes
within cubes of data. Each side of a cube is a dimension.
   Multi-dimensional databases are a compact and easy-to-understand way of visualizing
and manipulating data elements that have many inter-relationships. The cube can be expanded
to include another dimension, for example, the number of sales staff in each city. The cube
supports matrix arithmetic, which allows the cube to present the average revenue per sales
staff by simply performing a single matrix operation on all appropriate cells of the cube
(Average Revenue per Member of Sales Staff = Total Revenue/Number of Sales Staff).
   The response time of a multi-dimensional query depends on how many cells have to
be added on-the-fly. As the number of dimensions increases, the number of the cube’s
cells increases exponentially. However, the majority of multi-dimensional queries deal
with summarized, high-level data. Therefore, the solution to building an efficient multi-
dimensional database is to pre-aggregate (consolidate) all logical subtotals and totals along
all dimensions. This pre-aggregation can be especially valuable, as typical dimensions are
hierarchical in nature. For example, the time dimension may contain hierarchies for years,
quarters, months, weeks, and days, and the location dimension may contain branch office,
area, city, and country. Having the predefined hierarchy within dimensions allows for logi-
cal pre-aggregation and, conversely, allows for a logical ‘drill-down’, for example, from
annual revenues, to quarterly revenues, to monthly revenues.
                                             www.it-ebooks.info


1210         |   Chapter 33 z OLAP
Figure 33.1
Multi-dimensional
data viewed in:             Multi-dimensional OLAP supports common analytical operations, such as: consolida-
(a) three-field table;   tion, drill-down, and ‘slicing and dicing’.
(b) two-dimensional
matrix; (c) four-field   n  Consolidation involves the aggregation of data such as simple ‘roll-ups’ or complex
table; (d) three-           expressions involving interrelated data. For example, branch offices can be rolled up
dimensional cube.           to cities, and cities rolled up to countries.
                                                        www.it-ebooks.info


                                                                                      33.4 OLAP Tools | 1211
n  Drill-down is the reverse of consolidation and involves displaying the detailed data that
   comprises the consolidated data.
n  Slicing and dicing (also called pivoting) refers to the ability to look at the data from
   different viewpoints. For example, one slice of the revenue data may display all revenue
   generated per type of property within cities. Another slice may display all revenue
   generated by branch office within each city. Slicing and dicing is often performed along
   a time axis in order to analyze trends and find patterns.
Multi-dimensional OLAP servers have the ability to store multi-dimensional data in a
compressed form. This is accomplished by dynamically selecting physical storage organ-
izations and compression techniques that maximize space utilization. Dense data (that is,
data that exists for a high percentage of cells) can be stored separately from sparse data
(that is, a significant percentage of cells are empty). For example, certain branch offices
may only sell particular types of property, so that a percentage of cells that relate property
type to branch office may be empty and therefore sparse. Another kind of sparse data
is created when many cells contain duplicate data. For example, where there are large
numbers of branch offices in each major city of Great Britain, the cells holding the city
values will be duplicated many times over. The ability of a multi-dimensional DBMS to
omit empty or repetitive cells can greatly reduce the size of the cube and the amount of
processing. By optimizing space utilization, OLAP servers can minimize physical storage
requirements, thus making it possible to analyze exceptionally large amounts of data.
It also makes it possible to load more data into computer memory, which helps to
significantly improve performance by minimizing disk I/O. Although the argument for
specialized OLAP servers is persuasive, in the following section we also describe how
relational database systems are meeting the demands of OLAP.
   In summary, pre-aggregation, dimensional hierarchy, and sparse data management can
significantly reduce the size of the database and the need to calculate values. Such a design
obviates the need for multi-table joins and provides quick and direct access to arrays of
data, thus significantly speeding up execution of multi-dimensional queries. Although the
argument for specialized OLAP servers is persuasive, in the following sections we also
describe how relational database systems are meeting the demands of OLAP.
OLAP Tools                                                                                       33.4
There are many varieties of OLAP tools available in the marketplace. This choice has
resulted in some confusion, with much debate regarding what OLAP actually means to a
potential buyer and in particular what are the available architectures for OLAP tools. In
this section we first describe the generic rules for OLAP tools, without reference to a par-
ticular architecture, and then discuss the important characteristics, architecture, and issues
associated with each of the four main categories of commercially available OLAP tools.
Codd’s Rules for OLAP Tools                                                                     33.4.1
In 1993, E.F. Codd formulated twelve rules as the basis for selecting OLAP tools. The pub-
lication of these rules was the outcome of research carried out on behalf of Arbor Software
                                            www.it-ebooks.info


1212 | Chapter 33 z OLAP
                                      Table 33.2   Codd’s rules for OLAP tools.
                                       1.   Multi-dimensional conceptual view
                                       2.   Transparency
                                       3.   Accessibility
                                       4.   Consistent reporting performance
                                       5.   Client–server architecture
                                       6.   Generic dimensionality
                                       7.   Dynamic sparse matrix handling
                                       8.   Multi-user support
                                       9.   Unrestricted cross-dimensional operations
                                      10.   Intuitive data manipulation
                                      11.   Flexible reporting
                                      12.   Unlimited dimensions and aggregation levels
               (the creators of Essbase) and has resulted in a formalized redefinition of the requirements
               for OLAP tools. Codd’s rules for OLAP are listed in Table 33.2 (Codd et al., 1993).
               (1) Multi-dimensional conceptual view
               OLAP tools should provide users with a multi-dimensional model that corresponds to users’
               views of the enterprise and is intuitively analytical and easy to use. Interestingly, this rule is
               given various levels of support by vendors of OLAP tools who argue that a multi-dimensional
               conceptual view of data can be delivered without multi-dimensional storage.
               (2) Transparency
               The OLAP technology, the underlying database and architecture, and the possible hetero-
               geneity of input data sources should be transparent to users. This requirement is to preserve
               the user’s productivity and proficiency with familiar frontend environments and tools.
               (3) Accessibility
               The OLAP tool should be able to access data required for the analysis from all hetero-
               geneous enterprise data sources such as relational, non-relational, and legacy systems.
               (4) Consistent reporting performance
               As the number of dimensions, levels of aggregations, and the size of the database
               increases, users should not perceive any significant degradation in performance. There
               should be no alteration in the way the key figures are calculated. The system models
               should be robust enough to cope with changes to the enterprise model.
               (5) Client–server architecture
               The OLAP system should be capable of operating efficiently in a client–server environ-
               ment. The architecture should provide optimal performance, flexibility, adaptability,
               scalability, and interoperability.
                                            www.it-ebooks.info


                                                                                   33.4 OLAP Tools | 1213
(6) Generic dimensionality
Every data dimension must be equivalent in both structure and operational capabilities.
In other words, the basic structure, formulae, and reporting should not be biased towards
any one dimension.
(7) Dynamic sparse matrix handling
The OLAP system should be able to adapt its physical schema to the specific analytical
model that optimizes sparse matrix handling to achieve and maintain the required level
of performance. Typical multi-dimensional models can easily comprise millions of cell
references, many of which may have no appropriate data at any one point in time. These
nulls should be stored in an efficient way and not have any adverse impact on the accuracy
or speed of data access.
(8) Multi-user support
The OLAP system should be able to support a group of users working concurrently on the
same or different models of the enterprise’s data.
(9) Unrestricted cross-dimensional operations
The OLAP system must be able to recognize dimensional hierarchies and automatically
perform associated roll-up calculations within and across dimensions.
(10) Intuitive data manipulation
Slicing and dicing (pivoting), drill-down, and consolidation (roll-up), and other manipula-
tions should be accomplished via direct ‘point-and-click’ and ‘drag-and-drop’ actions on
the cells of the cube.
(11) Flexible reporting
The ability to arrange rows, columns, and cells in a fashion that facilitates analysis by
intuitive visual presentation of analytical reports must exist. Users should be able to
retrieve any view of the data that they require.
(12) Unlimited dimensions and aggregation levels
Depending on business requirements, an analytical model may have numerous dimensions,
each having multiple hierarchies. The OLAP system should not impose any artificial
restrictions on the number of dimensions or aggregation levels.
Since the publication of Codd’s rules for OLAP, there have been many proposals for the
rules to be redefined or extended. For example, some proposals state that in addition to
the twelve rules, commercial OLAP tools should also include comprehensive database
management tools, the ability to drill down to detail (source record) level, incremental
database refresh, and an SQL interface to the existing enterprise environment. For a dis-
cussion on the rules/features of OLAP that followed Codd’s initial publication in 1993, the
interested reader is referred to Thomsen (1997) and Pendse (2000).
                                           www.it-ebooks.info


1214       |   Chapter 33 z OLAP
        33.4.2 Categories of OLAP Tools
                       OLAP tools are categorized according to the architecture used to store and process multi-
                       dimensional data. There are four main categories of OLAP tools as defined by Berson and
                       Smith (1997) and Pendse and Creeth (2001) including:
                       n  Multi-dimensional OLAP (MOLAP);
                       n  Relational OLAP (ROLAP);
                       n  Hybrid OLAP (HOLAP);
                       n  Desktop OLAP (DOLAP).
                       Multi-dimensional OLAP (MOLAP)
                       MOLAP tools use specialized data structures and multi-dimensional database manage-
                       ment systems (MDDBMSs) to organize, navigate, and analyze data. To enhance query
                       performance the data is typically aggregated and stored according to predicted usage.
                       MOLAP data structures use array technology and efficient storage techniques that mini-
                       mize the disk space requirements through sparse data management. MOLAP tools provide
                       excellent performance when the data is used as designed, and the focus is on data for a
                       specific decision-support application. Traditionally, MOLAP tools require a tight coupling
                       of the application layer and presentation layer. However, recent trends segregate the
                       OLAP from the data structures through the use of published application programming
                       interfaces (APIs). The typical architecture for MOLAP tools is shown in Figure 33.2.
                          The development issues associated with MOLAP are as follows:
                       n  Only a limited amount of data can be efficiently stored and analyzed. The underlying
                          data structures are limited in their ability to support multiple subject areas and to pro-
                          vide access to detailed data. (Some products address this problem using mechanisms
                          that enable the MOLAP tools to access detailed data maintained in an RDBMS.)
                       n  Navigation and analysis of data are limited because the data is designed according to
                          previously determined requirements. Data may need to be physically reorganized to
                          optimally support new requirements.
Figure 33.2
Architecture for
MOLAP tools.
                                                    www.it-ebooks.info


                                                                                  33.4 OLAP Tools     |   1215
n  MOLAP products require a different set of skills and tools to build and maintain the
   database, thus increasing the cost and complexity of support.
Relational OLAP (ROLAP)
Relational OLAP (ROLAP) is the fastest-growing type of OLAP tool. This growth is
in response to users’ demands to analyze ever-increasing amounts of data and due to the
realization that users cannot store all the data they require in MOLAP databases. ROLAP
supports RDBMS products through the use of a metadata layer, thus avoiding the require-
ment to create a static multi-dimensional data structure. This facilitates the creation of
multiple multi-dimensional views of the two-dimensional relation. To improve perfor-
mance, some ROLAP products have enhanced SQL engines to support the complexity of
multi-dimensional analysis, while others recommend, or require, the use of highly denor-
malized database designs such as the star schema (see Section 32.2). The typical architec-
ture for ROLAP tools is shown in Figure 33.3.
   The development issues associated with ROLAP technology are as follows:
n  Performance problems associated with the processing of complex queries that require
   multiple passes through the relational data.
n  Development of middleware to facilitate the development of multi-dimensional
   applications, that is software that converts the two-dimensional relation into a multi-
   dimensional structure.
n  Development of an option to create persistent multi-dimensional structures, together
   with facilities to assist in the administration of these structures.
Hybrid OLAP (HOLAP)
Hybrid OLAP (HOLAP) tools provide limited analysis capability, either directly against
RDBMS products, or by using an intermediate MOLAP server. HOLAP tools deliver
selected data directly from the DBMS or via a MOLAP server to the desktop (or local
server) in the form of a data cube, where it is stored, analyzed, and maintained locally.
Vendors promote this technology as being relatively simple to install and administer with
reduced cost and maintenance. The typical architecture for HOLAP tools is shown in
Figure 33.4.
                                                                                            Figure 33.3
                                                                                            Architecture for
                                                                                            ROLAP tools.
                                             www.it-ebooks.info


1216       |   Chapter 33 z OLAP
Figure 33.4
Architecture for
HOLAP tools.
                          The issues associated with HOLAP tools are as follows:
                       n  The architecture results in significant data redundancy and may cause problems for net-
                          works that support many users.
                       n  Ability of each user to build a custom data cube may cause a lack of data consistency
                          among users.
                       n  Only a limited amount of data can be efficiently maintained.
                       Desktop OLAP (DOLAP)
                       An increasingly popular category of OLAP tools is Desktop OLAP (DOLAP). DOLAP
                       tools store the OLAP data in client-based files and support multi-dimensional processing
                       using a client multi-dimensional engine. DOLAP requires that relatively small extracts of
                       data are held on client machines. This data may be distributed in advance or on demand
                       (possibly through the Web). As with multi-dimensional databases on the server, OLAP
                       data may be held on disk or in RAM, however, some DOLAP products allow only read
                       access. Most vendors of DOLAP exploit the power of desktop PC to perform some, if not
                       most, multi-dimensional calculations.
                          The administration of a DOLAP database is typically performed by a central server or
                       processing routine that prepares data cubes or sets of data for each user. Once the basic
                       processing is done, each user can then access their portion of the data. The typical archi-
                       tecture for DOLAP tools is shown in Figure 33.5.
                          The development issues associated with DOLAP are as follows:
                       n  Provision of appropriate security controls to support all parts of the DOLAP environ-
                          ment. Since the data is physically extracted from the system, security is generally imple-
                          mented by limiting the information compiled into each cube. Once each cube is
                          uploaded to the user’s desktop, all additional metadata becomes the property of the
                          local user.
                       n  Reduction in the effort involved in deploying and maintaining the DOLAP tools. Some
                          DOLAP vendors now provide a range of alternative ways of deploying OLAP data such
                          as through e-mail, the Web, or using traditional client–server architecture.
                       n  Current trends are towards thin client machines.
                                                    www.it-ebooks.info


                                                        33.5 OLAP Extensions to the SQL Standard     |   1217
                                                                                           Figure 33.5
                                                                                           Architecture for
                                                                                           DOLAP tools.
OLAP Extensions to the SQL Standard                                                          33.5
In Chapters 5 and 6 we learnt that the advantages of SQL include that it is easy to learn,
non-procedural, free-format, DBMS-independent, and that it is a recognized international
standard. However, a major limitation of SQL for business analysts has been the difficulty
of using SQL to answer routinely asked business queries such as computing the percent-
age change in values between this month and a year ago or to compute moving averages,
cumulative sums, and other statistical functions. In answer to this limitation, ANSI has
adopted a set of OLAP functions as an extension to SQL that will enable these calculations
as well as many others that used to be impractical or even impossible within SQL. IBM
and Oracle jointly proposed these extensions early in 1999 and they now form part of the
current SQL standard, namely SQL:2003.
   The extensions are collectively referred to as the ‘OLAP package’ and include the fol-
lowing features of the SQL language as specified in the SQL Feature Taxonomy Annex of
the various parts of ISO/IEC 9075-2 (ISO, 2003a):
n  Feature T431, ‘Extended Grouping capabilities’;
n  Feature T611, ‘Extended OLAP operators’.
   In this section we discuss the Extended Grouping capabilities of the OLAP package by
demonstrating two examples of functions that form part of this feature, namely ROLLUP
and CUBE. We then discuss the Extended OLAP operators of the OLAP package by
demonstrating two examples of functions that form part of this feature, namely moving
                                          www.it-ebooks.info


1218  | Chapter 33 z OLAP
                window aggregations and ranking. To more easily demonstrate the usefulness of these
                OLAP functions it is necessary to use examples taken from an extended version of the
                DreamHome case study.
                   For full details on the OLAP package of the current SQL standard, the interested reader
                is referred to the ANSI Web site at www.ansi.org.
     33.5.1 Extended Grouping Capabilities
                Aggregation is a fundamental part of OLAP. To improve aggregation capabilities the SQL
                standard provides extensions to the GROUP BY clause such as the ROLLUP and CUBE
                functions.
                   ROLLUP supports calculations using aggregations such as SUM, COUNT, MAX, MIN,
                and AVG at increasing levels of aggregation, from the most detailed up to a grand total.
                CUBE is similar to ROLLUP, enabling a single statement to calculate all possible com-
                binations of aggregations. CUBE can generate the information needed in cross-tabulation
                reports with a single query.
                   ROLLUP and CUBE extensions specify exactly the groupings of interest in the GROUP
                BY clause and produces a single result set that is equivalent to a UNION ALL of differ-
                ently grouped rows. In the following sections we describe and demonstrate the ROLLUP
                and CUBE grouping functions in more detail.
                ROLLUP extension to GROUP BY
                ROLLUP enables a SELECT statement to calculate multiple levels of subtotals across a
                specified group of dimensions. ROLLUP appears in the GROUP BY clause in a SELECT
                statement using the following format:
                     SELECT . . . GROUP BY ROLLUP(columnList)
                   ROLLUP creates subtotals that roll up from the most detailed level to a grand total,
                following a column list specified in the ROLLUP clause. ROLLUP first calculates the
                standard aggregate values specified in the GROUP BY clause and then creates progres-
                sively higher-level subtotals, moving through the column list until finally completing with
                a grand total.
                   ROLLUP creates subtotals at n + 1 levels, where n is the number of grouping columns.
                For instance, if a query specifies ROLLUP on grouping columns of propertyType, yearMonth,
                and city (n = 3), the result set will include rows at 4 aggregation levels.
                   We demonstrate the usefulness of ROLLUP in the following example.
                Example 33.1 Using the ROLLUP group function
                Show the totals for sales of flats or houses by branch offices located in Aberdeen,
                Edinburgh, or Glasgow for the months of September and October of 2004.
                In this example, we require to first identify branch offices in the cities of Aberdeen,
                Edinburgh, and Glasgow and then to aggregate the total sales of flats and houses by these
                offices in each city for September and October of 2004.
                                              www.it-ebooks.info


                                                         33.5 OLAP Extensions to the SQL Standard | 1219
    To answer this query requires that we must extend the DreamHome case study to
include a new table called PropertySale, which has four attributes, namely branchNo,
propertyNo, yearMonth, and saleAmount. This table represents the sale of each property at
each branch. This query also requires access to the Branch and PropertyForSale tables
described earlier in Figure 3.3. Note that both the Branch and PropertyForSale tables have a
column called city. To simplify this example and the others that follow, we change the
name of the city column in the PropertyForRent table to pcity. The format of the query using
the ROLLUP function is:
       SELECT propertyType, yearMonth, city, SUM(saleAmount) AS sales
       FROM Branch, PropertyFor Sale, PropertySale
       WHERE Branch.branchNo = PropertySale.branchNo
       AND PropertyForSale.propertyNo = PropertySale.propertyNo
       AND PropertySale.yearMonth IN (‘2004-08’, ‘2004-09’)
       AND Branch.city IN (‘Aberdeen’, ‘Edinburgh’, ‘Glasgow’)
         GROUP BY ROLLUP(propertyType, yearMonth, city);
The output for this query is shown in Table 33.3. Note that results do not always add up,
due to rounding. This query returns the following sets of rows:
n   Regular aggregation rows that would be produced by GROUP BY without using
    ROLLUP.
Table 33.3   Results table for Example 33.1.
  propertyType                   yearMonth                 city                    sales
  flat                           2004-08                   Aberdeen                 115432
  flat                           2004-08                   Edinburgh                236573
  flat                           2004-08                   Glasgow                    7664
  flat                           2004-08                                            359669
  flat                           2004-09                   Aberdeen                 123780
  flat                           2004-09                   Edinburgh                323100
  flat                           2004-09                   Glasgow                    8755
  flat                           2004-09                                            455635
  flat                                                                              815304
  house                          2004-08                   Aberdeen                  77987
  house                          2004-08                   Edinburgh                135670
  house                          2004-08                   Glasgow                    4765
  house                          2004-08                                            218422
  house                          2004-09                   Aberdeen                  76321
  house                          2004-09                   Edinburgh                166503
  house                          2004-09                   Glasgow                    4889
  house                          2004-09                                            247713
  house                                                                             466135
                                                                                   1281439
                                             www.it-ebooks.info


1220 | Chapter 33 z OLAP
               n  First-level subtotals aggregating across city for each combination of propertyType and
                  yearMonth.
               n  Second-level subtotals aggregating across yearMonth and city for each propertyType
                  value.
               n  A grand total row.
               CUBE extension to GROUP BY
               CUBE takes a specified set of grouping columns and creates subtotals for all of the pos-
               sible combinations. CUBE appears in the GROUP BY clause in a SELECT statement
               using the following format:
                    SELECT . . . GROUP BY CUBE(columnList)
               In terms of multi-dimensional analysis, CUBE generates all the subtotals that could be
               calculated for a data cube with the specified dimensions. For example, if we speci-
               fied CUBE(propertyType, yearMonth, city), the result set will include all the values that are
               included in an equivalent ROLLUP statement plus additional combinations. For instance,
               in Example 33.1 the city totals for combined property types are not calculated by a
               ROLLUP(propertyType, yearMonth, city) clause, but are calculated by a CUBE(propertyType,
               yearMonth, city) clause. If n columns are specified for a CUBE, there will be 2n combina-
               tions of subtotals returned. This example gives an example of a three-dimension cube.
               When to use CUBE
               CUBE can be used in any situation requiring cross-tabular reports. The data needed
               for cross-tabular reports can be generated with a single SELECT using CUBE. Like
               ROLLUP, CUBE can be helpful in generating summary tables.
                  CUBE is typically most suitable in queries that use columns from multiple dimensions
               rather than columns representing different levels of a single dimension. For instance, a
               commonly requested cross-tabulation might need subtotals for all the combinations of
               propertyType, yearMonth, and city. These are three independent dimensions, and analysis of
               all possible subtotal combinations is commonplace. In contrast, a cross-tabulation show-
               ing all possible combinations of year, month, and day would have several values of limited
               interest, because there is a natural hierarchy in the time dimension.
                  We demonstrate the usefulness of the CUBE function in the following example.
               Example 33.2 Using the CUBE group function
               Show all possible subtotals for sales of properties by branches offices in Aberdeen,
               Edinburgh, and Glasgow for the months of September and October of 2004.
               We replace the ROLLUP function shown in the SQL query of Example 33.1 with the
               CUBE function. The format of this query is:
                    SELECT propertyType, yearMonth, city, SUM(saleAmount) AS sales
                    FROM Branch, PropertyFor Sale, PropertySale
                    WHERE Branch.branchNo = PropertySale.branchNo
                                             www.it-ebooks.info


                                                        33.5 OLAP Extensions to the SQL Standard | 1221
      AND PropertyForSale.propertyNo = PropertySale.propertyNo
      AND PropertySale.yearMonth IN (‘2004-08’, ‘2004-09’)
      AND Branch.city IN (‘Aberdeen’, ‘Edinburgh’, ‘Glasgow’)
       GROUP BY CUBE(propertyType, yearMonth, city);
The output is shown in Table 33.4.
Table 33.4  Results table for Example 33.2.
 propertyType                   yearMonth                 city                  sales
 flat                           2004-08                   Aberdeen                115432
 flat                           2004-08                   Edinburgh               236573
 flat                           2004-08                   Glasgow                   7664
 flat                           2004-08                                           359669
 flat                           2004-09                   Aberdeen                123780
 flat                           2004-09                   Edinburgh               323100
 flat                           2004-09                   Glasgow                   8755
 flat                           2004-09                                           455635
 flat                                                     Aberdeen                239212
 flat                                                     Edinburgh               559673
 flat                                                     Glasgow                  16419
 flat                                                                             815304
 house                          2004-08                   Aberdeen                 77987
 house                          2004-08                   Edinburgh               135670
 house                          2004-08                   Glasgow                   4765
 house                          2004-08                                           218422
 house                          2004-09                   Aberdeen                 76321
 house                          2004-09                   Edinburgh               166503
 house                          2004-09                   Glasgow                   4889
 house                          2004-09                                           247713
 house                                                    Aberdeen                154308
 house                                                    Edinburgh               302173
 house                                                    Glasgow                   9654
 house                                                                            466135
                                2004-08                   Aberdeen                193419
                                2004-08                   Edinburgh               372243
                                2004-08                   Glasgow                  12429
                                2004-08                                           578091
                                2004-09                   Aberdeen                200101
                                2004-09                   Edinburgh               489603
                                2004-09                   Glasgow                  13644
                                2004-09                                           703348
                                                          Aberdeen                393520
                                                          Edinburgh               861846
                                                          Glasgow                  26073
                                                                                1281439
                                            www.it-ebooks.info


1222  | Chapter 33 z OLAP
                   The rows shown in bold are those that are common to the results tables produced for both
                the ROLLUP (see Table 33.3) and the CUBE functions. However, the CUBE(propertyType,
                yearMonth, city) clause, where n = 3, produces 23 = 8 levels of aggregation, whereas in
                Example 33.1, the ROLLUP(propertyType, yearMonth, city) clause, where n = 3, produced
                only 3 + 1 = 4 levels of aggregation.
     33.5.2 Elementary OLAP Operators
                The Elementary OLAP operators of the OLAP package of the SQL standard supports a
                variety of operations such as rankings and window calculations. Ranking functions include
                cumulative distributions, percent rank, and N-tiles. Windowing allows the calculation of
                cumulative and moving aggregations using functions such as SUM, AVG, MIN, and
                COUNT. In the following sections we describe and demonstrate the ranking and window-
                ing calculations in more detail.
                Ranking functions
                A ranking function computes the rank of a record compared to other records in the dataset
                based on the values of a set of measures. There are various types of ranking functions,
                including RANK and DENSE_RANK. The syntax for each ranking function is:
                     RANK( ) OVER (ORDER BY columnList)
                     DENSE_RANK( ) OVER (ORDER BY columnList)
                The syntax shown is incomplete but sufficient to discuss and demonstrate the useful-
                ness of these functions. The difference between RANK and DENSE_RANK is that
                DENSE_RANK leaves no gaps in the sequential ranking sequence when there are ties for
                a ranking. For example, if three branch offices tie for second place in terms of total prop-
                erty sales, DENSE_RANK identifies all three in second place with the next branch in third
                place. The RANK function also identifies three branches in second place, but the next
                branch is in fifth place. We demonstrate the usefulness of the RANK and DENSE_RANK
                functions in the following example.
                Example 33.3 Using the RANK and DENSE_RANK functions
                Rank the total sales of properties for branch offices in Edinburgh.
                We first calculate the total sales for properties at each branch office in Edinburgh and then
                rank the results. This query accesses the Branch and PropertySale tables. We demonstrate the
                difference in how the RANK and DENSE_RANK functions work in the following query:
                     SELECT branchNo, SUM(saleAmount) AS sales,
                     RANK() OVER (ORDER BY SUM(saleAmount)) DESC AS ranking,
                     DENSE_RANK() OVER (ORDER BY SUM(saleAmount)) DESC AS dense_ranking
                                              www.it-ebooks.info


                                                         33.5 OLAP Extensions to the SQL Standard | 1223
     FROM Branch, PropertySale
     WHERE Branch.branchNo = PropertySale.branchNo
     AND Branch.city = ‘Edinburgh’
        GROUP BY(branchNo);
The output is shown in Table 33.5.
Table 33.5  Results table for Example 33.3.
  branchNo                  sales                    ranking              dense_ranking
  B009                      120,000,000              1                    1
  B018                       92,000,000              2                    2
  B022                       92,000,000              2                    2
  B028                       92,000,000              2                    2
  B033                       45,000,000              5                    3
  B046                       42,000,000              6                    4
Windowing calculations
Windowing calculations can be used to compute cumulative, moving, and centered aggre-
gates. They return a value for each row in the table, which depends on other rows in the
corresponding window. For example, windowing can calculate cumulative sums, moving
sums, moving averages, moving min/max, as well as other statistical measurements.
These aggregate functions provide access to more than one row of a table without a self-
join and can be used only in the SELECT and ORDER BY clauses of the query.
   We demonstrate how windowing can be used to produce moving averages and sums in
the following example.
Example 33.4 Using windowing calculations
Show the monthly figures and three-month moving averages and sums for property
sales at branch office B003 for the first six months of 2004.
We first sum the property sales for each month of the first six months of 2004 at branch
office B003 and then use these figures to determine the three-month moving averages
and three-month moving sums. In other words, we calculate the moving average and
moving sum for property sales at branch B003 for the current month and preceding
two months. This query accesses the PropertySale table. We demonstrate the creation of
a three-month moving window using the ROWS 2 PRECEDING function in the follow-
ing query:
                                            www.it-ebooks.info


1224  | Chapter 33 z OLAP
                     SELECT yearMonth, SUM(saleAmount) AS monthlySales, AVG(SUM(saleAmount))
                     OVER (ORDER BY yearMonth, ROWS 2 PRECEDING) AS 3-month moving avg,
                     SUM(SUM(salesAmount)) OVER (ORDER BY yearMonth ROWS 2 PRECEDING)
                     AS 3-month moving sum
                     FROM PropertySale
                     WHERE branchNo = ‘B003’
                     AND yearMonth BETWEEN (‘2004-01’ AND ‘2004-06’)
                        GROUP BY yearMonth
                        ORDER BY yearMonth;
                The output is shown in Table 33.6.
                Table 33.6 Results table for Example 33.4.
                  yearMonth          monthlySales       3-Month Moving Avg            3-Month Moving Sum
                  2004-01            210000             210000                         210000
                  2004-02            350000             280000                         560000
                  2004-03            400000             320000                         960000
                  2004-04            420000             390000                        1170000
                  2004-05            440000             420000                        1260000
                  2004-06            430000             430000                        1290000
                Note that the first two rows for the three-month moving average and sum calculations
                in the results table are based on a smaller interval size than specified because the window
                calculation cannot reach past the data retrieved by the query. It is therefore necessary to
                consider the different window sizes found at the borders of result sets. In other words, we
                may need to modify the query to include exactly what we want.
                   Oracle plays an important part in the continuing development and improvement of the
                SQL standard. In fact, many of the new OLAP features of SQL:2003 has been supported
                by Oracle since version 8/8i. In the following section, we describe briefly how Oracle9i
                supports OLAP.
     33.6       Oracle OLAP
                In large data warehouse environments, many different types of analysis can occur as part
                of building a platform to support business intelligence. In addition to traditional SQL
                queries, users require to perform more advanced analytical operations on the data. Two
                major types of analysis are Online Analytical Processing (OLAP) and data mining.
                This section describes how Oracle provides OLAP as an important component of Oracle’s
                                             www.it-ebooks.info


                                                                                 33.6 Oracle OLAP | 1225
business intelligence platform (Oracle Corporation, 2004h, i). In the following chapter we
describe how Oracle supports data mining.
Oracle OLAP Environment                                                                      33.6.1
The value of the data warehouse is its ability to support business intelligence. To date,
standard reporting and ad hoc query and reporting applications have run directly from
relational tables while more sophisticated business intelligence applications have used
specialized analytical databases. These specialized analytical databases typically provide
support for complex multi-dimensional calculations and predictive functions; however,
they rely on replicating large volumes of data into proprietary databases.
   Replication of data into proprietary analytical databases is extremely expensive.
Additional hardware is required to run analytical databases and store replicated data.
Additional database administrators are required to manage the system. The replication
process often causes a significant lag between the time data becomes available in the data
warehouse and when it is staged for analysis in the analytical database. Latency caused by
data replication can significantly affect the value of the data.
   Oracle OLAP provides support for business intelligence applications without the need
for replicating large volumes of data in specialized analytical databases. Oracle OLAP
allows applications to support complex multi-dimensional calculations directly against the
data warehouse. The result is a single database that is more manageable, more scalable,
and accessible to the largest number of applications.
   Business intelligence applications are only useful when they are easily accessed. To
support access by large, distributed user communities, Oracle OLAP is designed for the
Internet. The Oracle9i Java OLAP API provides a modern Internet-ready API that allows
application developers to build Java applications, applets, servlets, and JSPs that can be
deployed using a variety of devices such as PCs and workstations, Web browsers, PDAs,
and Web-enabled mobile phones.
Platform for Business Intelligence Applications                                              33.6.2
Oracle9i Database provides a platform for business intelligence applications. The com-
ponents of the platform include the Oracle9i Database and Oracle OLAP as a facility
within Oracle9i Database. This platform provides:
n  a complete range of analytical functions, including multi-dimensional and predictive
   functions;
n  support for rapid query response times such as those that are normally associated with
   specialized analytical databases;
n  a scalable platform for storing and analyzing multi-terabyte data sets;
n  a platform that is open to both multi-dimensional and SQL-based applications;
n  support for Internet-based applications.
                                            www.it-ebooks.info


1226  | Chapter 33 z OLAP
     33.6.3 Oracle9i Database
                The Oracle9i Database provides the foundation for Oracle OLAP by providing a scalable
                and secure data store, summary management facilities, metadata, SQL analytical func-
                tions, and high availability features.
                   Scalability features that provide support for multi-terabyte data warehouses include:
                n  partitioning, which allows objects in the data warehouse to be broken down into smaller
                   physical components that can then be managed independently and in parallel;
                n  parallel query execution, which allows the database to use multiple processes to satisfy
                   a single Java OLAPI API query;
                n  support for NUMA and clustered systems, which allows organizations to use and man-
                   age large hardware systems effectively;
                n  Oracle’s Database Resource Manager, which helps manage large and diverse user com-
                   munities by controlling the amounts of resources each user type is allowed to use.
                Security
                Security is critical to the data warehouse. To provide the strongest possible security and
                to minimize administrative overhead, all security policies are enforced within the data
                warehouse. Users are authenticated in the Oracle database using database authentication
                or Oracle Internet Directory. Access to elements of the multi-dimensional data model is
                controlled through grants and privileges in the Oracle database. Cell level access to data is
                controlled in the Oracle database using Oracle’s Virtual Private Database feature.
                Summary management
                Materialized views provide facilities for effectively managing data within the data ware-
                house. As compared with summary tables, materialized views offer several advantages:
                n  they are transparent to applications and users;
                n  they manage staleness of data;
                n  they can automatically update themselves when source data changes.
                Like Oracle tables, materialized views can be partitioned and maintained in parallel.
                Unlike proprietary multi-dimensional cubes, data in materialized views is equally accessi-
                ble by all applications using the data warehouse.
                Metadata
                All metadata is stored in the Oracle database. Low-level objects such as dimensions, tables,
                and materialized views are defined directly from the Oracle data dictionary, while higher-
                level OLAP objects are defined in the OLAP catalog. The OLAP catalog contains objects
                such as Cubes and Measure folders as well as extensions to the definitions of other objects
                such as dimensions. The OLAP catalog fully defined the dimensions and facts and thus
                completes the definition of the star schema.
                                              www.it-ebooks.info


                                                                                            33.6 Oracle OLAP | 1227
SQL analytical functions
Oracle has enhanced SQL’s analytical processing capabilities by introducing a new fam-
ily of analytical SQL functions. These analytical functions include the ability to calculate:
n  rankings and percentiles;
n  moving window calculations;
n  lag/lead analysis;
n  first/last analysis;
n  linear regression statistics.
Ranking functions include cumulative distributions, percent rank, and N-tiles. Moving
window calculations identify moving and cumulative aggregations, such as sums and
averages. Lag/lead analysis enables direct inter-row references to support the calculation
for period-to-period changes. First/last analysis identifies the first or last value in an
ordered group. Linear regression functions support the fitting of an ordinary-least-squares
regression line to a set of number pairs. This can be used as both aggregate functions and
windowing or reporting functions. The SQL analytical functions supported by Oracle are
classified and described briefly in Table 33.7.
   To enhance performance, analytical functions can be parallelized: multiple pro-
cesses can simultaneously execute all of these statements. These capabilities make cal-
culations easier and more efficient, thereby enhancing database performance, scalability,
and simplicity.
Table 33.7     Oracle SQL analytical functions.
  Type                   Used for
  Ranking                Calculating ranks, percentiles, and N-tiles of the values in a result set.
  Windowing              Calculating cumulative and moving aggregates. Works with these
                         functions: SUM, AVG, MIN, MAX, COUNT, VARIANCE, STDDEV,
                         FIRST_VALUE, LAST_VALUE, and new statistical functions.
  Reporting              Calculating shares, for example market share. Works with these
                         functions: SUM, AVG, MIN, MAX, COUNT (with/without
                         DISTINCT), VARIANCE, STDDEV, RATIO_TO_REPORT, and new
                         statistical functions.
  LAG/LEAD               Finding a value in a row a specified number of rows from a current row.
  FIRST/LAST             First or last value in an ordered group.
  Linear Regression      Calculating linear regression and other statistics (slope, intercept,
                         and so on).
  Inverse Percentile     The value in a data set that corresponds to a specified percentile.
  Hypothetical Rank      The rank or percentile that a row would have if inserted into a specified
  and Distribution       data set.
                                                www.it-ebooks.info


1228  | Chapter 33 z OLAP
                Disaster recovery
                Oracle’s disaster recovery features protects data in the data warehouse. Key features
                include:
                n  Oracle Data Guard, a comprehensive standby database disaster recovery solution;
                n  redo logs and the recovery catalog;
                n  backup and restore operations that are fully integrated with Oracle’s partition features;
                n  support for incremental backup and recovery.
     33.6.4 Oracle OLAP
                Oracle OLAP, an integrated part of Oracle9i Database, provides support for multi-
                dimensional calculations and predictive functions. Oracle OLAP supports both the Oracle
                relational tables and analytic workspaces (a multi-dimensional data type). Key features of
                Oracle OLAP include:
                n  the ability to support complex, multi-dimensional calculations;
                n  support for predictive functions such as forecasts, models, non-additive aggregations
                   and allocations, and scenario management (what-if);
                n  a Java OLAP API;
                n  integrated OLAP administration.
                Multi-dimensional calculations allow the user to analyze data across dimensions. For
                example, a user could ask for ‘The top ten products for each of the top ten customers
                during a rolling six month time period based on growth in dollar sales’. In this query a
                product ranking is nested within a customer ranking, data is analyzed across a number
                of time periods and a virtual measure. These types of queries are resolved directly in the
                relational database.
                   Predictive functions allow applications to answer questions such as ‘How profitable
                will the company be next quarter?’ and ‘How many items should be manufactured this
                month?’ Predictive functions are resolved within a multi-dimensional data type known as
                an analytic workspace using the Oracle OLAP DML.
                   Oracle OLAP uses a multi-dimensional data model that allows users to express queries
                in business terms (what products, what customers, what time periods, and what facts). The
                multi-dimensional model includes measures, cubes, dimensions, levels, hierarchies, and
                attributes.
                Java OLAP API
                The Oracle9i OLAP API is based on Java. As a result it is an object-oriented, platform-
                independent, and secure API that allows application developers to build Java applications,
                Java Applets, Java Servlets, and Java Server Pages (JSP) that can be deployed to large,
                distributed user communities over the Internet. Key features to the Java OLAP API
                include:
                                             www.it-ebooks.info


                                                                                 33.6 Oracle OLAP | 1229
n  encapsulation;
n  support for multi-dimensional calculations;
n  incremental query construction;
n  multi-dimensional cursors.
Performance                                                                                  33.6.5
Oracle9i Database eliminates the tradeoff between analytical complexity and support for
large databases. On smaller data sets (where specialized analytically databases typically
excel) Oracle9i provides query performance that is competitive with specialized multi-
dimensional databases. As databases grow larger and as more data must be accessed in
order to resolve queries, Oracle9i will continue to provide excellent query performance
while the performance of specialized analytical databases will typically degrade.
   Oracle9i Database achieves both performance and scalability through SQL that is highly
optimized for multi-dimensional queries and the Oracle database. Accessing cells of data
within the multi-dimensional model is a critical factor in providing query performance that
is competitive with specialized analytical databases. New features in the Oracle database
that provide support high performance random cell access and multi-dimensional queries
include:
n  bitmap join indexes which are used in the warehouse to pre-join dimension tables and
   fact tables and store the result in a single bitmap index;
n  grouping sets which allow Oracle to select data from multiple levels of summarization
   in a single select statement;
n  the WITH clause which allows Oracle to create temporary results and use these results
   within the query, thus eliminating the need for creating temporary tables;
n  SQL OLAP functions which provide highly concise means to express many OLAP
   functions;
n  automatic memory management features which provide the correct amounts of memory
   during memory-intensive tasks;
n  enhanced cursor sharing which eliminates the need to recompile queries when another,
   similar query has been run.
System Management                                                                            33.6.6
Oracle Enterprise Manager (OEM) provides a centralized, comprehensive management tool.
OEM enables administrators to monitor all aspects of the database, including Oracle OLAP.
Oracle Enterprise Manager provides management services to Oracle OLAP including:
n  instance, session, and configuration management;
n  data modeling;
n  performance monitoring;
n  job scheduling.
                                             www.it-ebooks.info


1230     |  Chapter 33 z OLAP
      33.6.7 System Requirements
                      Oracle OLAP is installed as part of the Oracle9i Database and imposes no additional
                      system requirements. Oracle OLAP can also be installed on a middle-tier system.
                      When installed on a middle-tier system, 128 MB of memory is required. When analytic
                      workspaces are used extensively, additional memory is recommended. The actual amount
                      of memory for use with analytic workspaces will vary with the application.
 Chapter Summary
 n Online Analytical Processing (OLAP) is the dynamic synthesis, analysis, and consolidation of large volumes
   of multi-dimensional data.
 n OLAP applications are found in widely divergent functional areas including budgeting, financial performance
   analysis, sales analysis and forecasting, market research analysis, and market/customer segmentation.
 n The key characteristics of OLAP applications include multi-dimensional views of data, support for complex
   calculations, and time intelligence.
 n OLAP database servers use multi-dimensional structures to store data and relationships between data. Multi-
   dimensional structures can be visualized as cubes of data, and cubes within cubes of data. Each side of the
   cube is considered a dimension.
 n Pre-aggregation, dimensional hierarchy, and sparse data management can significantly reduce the size
   of the database and the need to calculate values. These approaches remove the need for multi-table joins and
   provide quick and direct access to the arrays of data, thus significantly speeding up execution of the multi-
   dimensional queries.
 n E.F. Codd formulated twelve rules as the basis for selecting OLAP tools.
 n OLAP tools are categorized according to the architecture of the database providing the data for the purposes
   of analytical processing. There are four main categories of OLAP tools: Multi-dimensional OLAP
   (MOLAP), Relational OLAP (ROLAP), Hybrid OLAP (HOLAP), and Desktop OLAP (DOLAP).
 n The SQL:2003 standard supports OLAP functionality in the provision of extensions to grouping cap-
   abilities such as the CUBE and ROLLLUP functions and elementary operators such as moving windows and
   ranking functions.
                                                  www.it-ebooks.info


                                                                                                Exercises    |   1231
  Review Questions
  33.1 Discuss what Online Analytical Processing              33.6 Describe the architecture, characteristics, and
       (OLAP) represents.                                           issues associated with each of the following
  33.2 Discuss the relationship between data                        categories of OLAP tools:
       warehousing and OLAP.                                        (a) MOLAP,
  33.3 Describe OLAP applications and                               (b) ROLAP,
       identify the characteristics of such                         (c) HOLAP,
       applications.                                                (d) DOLAP.
  33.4 Describe the characteristics of multi-                 33.7 Discuss how OLAP functionality is provided by
       dimensional data and how this data can                       the ROLLUP and CUBE functions of the SQL
       be represented.                                              standard.
  33.5 Describe Codd’s rules for OLAP                         33.8 Discuss how OLAP functionality is provided by
       tools.                                                       elementary operators such as moving windows
                                                                    and ranking functions of the SQL standard.
Exercises
33.9   You are asked by the Managing Director of DreamHome to investigate and report on the applicability of
       OLAP for the organization. The report should describe the technology and provide a comparison with tradi-
       tional querying and reporting tools of relational DBMSs. The report should also identify the advantages and
       disadvantages, and any problem areas associated with implementing OLAP. The report should reach a fully
       justified set of conclusions on the applicability of OLAP for DreamHome.
33.10 Investigate whether your organization (such as your university/college or workplace) has invested in OLAP
       technologies and, if yes, whether the OLAP tool(s) forms part of a larger investment in business intelligence
       technologies. If possible, establish the reasons for the interest in OLAP, how the tools are being applied, and
       whether the promise of OLAP has been realized.
                                             www.it-ebooks.info


     Chapter
34Chapter Objectives
                             Data Mining
  In this chapter you will learn:
  n  The concepts associated with data mining.
  n  The main features of data mining operations, including predictive modeling,
     database segmentation, link analysis, and deviation detection.
  n  The techniques associated with the data mining operations.
  n  The process of data mining.
  n  Important characteristics of data mining tools.
  n  The relationship between data mining and data warehousing.
  n  How Oracle supports data mining.
In Chapter 31 we discussed that the increasing popularity of data warehousing (or more
commonly data marts) has been accompanied by greater demands by users for more
powerful access tools that provide advanced analytical capabilities. There are two main
types of access tools available to meet these demands, namely Online Analytical
Processing (OLAP) and data mining. In the previous chapter we described OLAP and in
this chapter we describe data mining.
  Structure of this Chapter
In Section 34.1 we discuss what data mining is and present examples of typical data min-
ing applications. In Section 34.2 we describe the main features of data mining operations
and their associated techniques. In Section 34.3 we describe the process of data mining. In
Section 34.4 we discuss the important characteristics of data mining tools and in Section
34.5 we examine the relationship between data mining and data warehousing. Finally, in
Section 34.6 we describe how Oracle supports data mining.
                             www.it-ebooks.info


                                                                        34.2 Data Mining Techniques | 1233
Data Mining                                                                                     34.1
Simply storing information in a data warehouse does not provide the benefits an organi-
zation is seeking. To realize the value of a data warehouse, it is necessary to extract the
knowledge hidden within the warehouse. However, as the amount and complexity of the
data in a data warehouse grows, it becomes increasingly difficult, if not impossible, for
business analysts to identify trends and relationships in the data using simple query and
reporting tools. Data mining is one of the best ways to extract meaningful trends and pat-
terns from huge amounts of data. Data mining discovers information within data ware-
houses that queries and reports cannot effectively reveal.
   There are numerous definitions of what data mining is, ranging from the broadest
definitions of any tool that enables users to access directly large amounts of data, to more
specific definitions such as tools and applications that perform statistical analysis on the
data. In this chapter, we use a more focused definition of data mining by Simoudis (1996):
  Data        The process of extracting valid, previously unknown, comprehensible, and
  mining      actionable information from large databases and using it to make crucial
              business decisions.
Data mining is concerned with the analysis of data and the use of software techniques for
finding hidden and unexpected patterns and relationships in sets of data. The focus of data
mining is to reveal information that is hidden and unexpected, as there is little value in
finding patterns and relationships that are already intuitive. Examining the underlying
rules and features in the data identifies the patterns and relationships.
   Data mining analysis tends to work from the data up, and the techniques that produce
the most accurate results normally require large volumes of data to deliver reliable con-
clusions. The analysis process starts by developing an optimal representation of the struc-
ture of sample data, during which time knowledge is acquired. This knowledge is then
extended to larger sets of data, working on the assumption that the larger data set has a
structure similar to the sample data.
   Data mining can provide huge paybacks for companies who have made a significant
investment in data warehousing. Although data mining is still a relatively new technology,
it is already used in a number of industries. Table 34.1 lists examples of applications of
data mining in retail/marketing, banking, insurance, and medicine.
Data Mining Techniques                                                                          34.2
There are four main operations associated with data mining techniques, which include pre-
dictive modeling, database segmentation, link analysis, and deviation detection. Although
any of the four major operations can be used for implementing any of the business
applications listed in Table 34.1, there are certain recognized associations between the
applications and the corresponding operations. For example, direct marketing strategies
are normally implemented using the database segmentation operation, while fraud detec-
tion could be implemented by any of the four operations. Further, many applications work
                                            www.it-ebooks.info


1234 | Chapter 34 z Data Mining
                               Table 34.1    Examples of data mining applications.
                               Retail/Marketing
                               Identifying buying patterns of customers
                               Finding associations among customer demographic characteristics
                               Predicting response to mailing campaigns
                               Market basket analysis
                               Banking
                               Detecting patterns of fraudulent credit card use
                               Identifying loyal customers
                               Predicting customers likely to change their credit card affiliation
                               Determining credit card spending by customer groups
                               Insurance
                               Claims analysis
                               Predicting which customers will buy new policies
                               Medicine
                               Characterizing patient behavior to predict surgery visits
                               Identifying successful medical therapies for different illnesses
               particularly well when several operations are used. For example, a common approach to
               customer profiling is to segment the database first and then apply predictive modeling to
               the resultant data segments.
                  Techniques are specific implementations of the data mining operations. However, each
               operation has its own strengths and weaknesses. With this in mind, data mining tools
               sometimes offer a choice of operations to implement a technique. In Table 34.2, we list the
               main techniques associated with each of the four main data mining operations (Cabena
               et al., 1997).
                         Table 34.2   Data mining operations and associated techniques.
                          Operations                                  Data mining techniques
                          Predictive modeling                         Classification
                                                                      Value prediction
                          Database segmentation                       Demographic clustering
                                                                      Neural clustering
                          Link analysis                               Association discovery
                                                                      Sequential pattern discovery
                                                                      Similar time sequence discovery
                          Deviation detection                         Statistics
                                                                      Visualization
                                             www.it-ebooks.info


                                                                         34.2 Data Mining Techniques       |   1235
   For a fuller discussion on data mining techniques and applications, the interested reader
is referred to Cabena et al. (1997).
Predictive Modeling                                                                             34.2.1
Predictive modeling is similar to the human learning experience in using observations to
form a model of the important characteristics of some phenomenon. This approach uses
generalizations of the ‘real world’ and the ability to fit new data into a general framework.
Predictive modeling can be used to analyze an existing database to determine some essen-
tial characteristics (model) about the data set. The model is developed using a supervised
learning approach, which has two phases: training and testing. Training builds a model
using a large sample of historical data called a training set, while testing involves trying
out the model on new, previously unseen data to determine its accuracy and physical
performance characteristics. Applications of predictive modeling include customer retention
management, credit approval, cross-selling, and direct marketing. There are two tech-
niques associated with predictive modeling: classification and value prediction, which are
distinguished by the nature of the variable being predicted.
Classification
Classification is used to establish a specific predetermined class for each record in a
database from a finite set of possible class values. There are two specializations of
classification: tree induction and neural induction. An example of classification using tree
induction is shown in Figure 34.1.
   In this example, we are interested in predicting whether a customer who is currently renting
property is likely to be interested in buying property. A predictive model has determined
that only two variables are of interest: the length of time the customer has rented property
and the age of the customer. The decision tree presents the analysis in an intuitive way.
The model predicts that those customers who have rented for more than two years and are over
25 years old are the most likely to be interested in buying property. An example of classi-
fication using neural induction is shown in Figure 34.2 using the same example as Figure 34.1.
                                                                                                Figure 34.1
                                                                                                An example of
                                                                                                classification using
                                                                                                tree induction.
                                             www.it-ebooks.info


1236         |  Chapter 34 z Data Mining
Figure 34.2
An example of
classification using
neural induction.
                            In this case, classification of the data is achieved using a neural network. A neural net-
                        work contains collections of connected nodes with input, output, and processing at each
                        node. Between the visible input and output layers may be a number of hidden processing
                        layers. Each processing unit (circle) in one layer is connected to each processing unit
                        in the next layer by a weighted value, expressing the strength of the relationship. The
                        network attempts to mirror the way the human brain works in recognizing patterns by
                        arithmetically combining all the variables associated with a given data point. In this way,
                        it is possible to develop nonlinear predictive models that ‘learn’ by studying combinations
                        of variables and how different combinations of variables affect different data sets.
                        Value prediction
                        Value prediction is used to estimate a continuous numeric value that is associated with a
                        database record. This technique uses the traditional statistical techniques of linear regres-
                        sion and nonlinear regression. As these techniques are well-established, they are relatively
                        easy to use and understand. Linear regression attempts to fit a straight line through a plot
                        of the data, such that the line is the best representation of the average of all observations
                        at that point in the plot. The problem with linear regression is that the technique only
                        works well with linear data and is sensitive to the presence of outliers (that is, data values
                        which do not conform to the expected norm). Although nonlinear regression avoids the
                        main problems of linear regression, it is still not flexible enough to handle all possible
                        shapes of the data plot. This is where the traditional statistical analysis methods and data
                        mining methods begin to diverge. Statistical measurements are fine for building linear
                        models that describe predictable data points; however, most data is not linear in nature.
                        Data mining requires statistical methods that can accommodate nonlinearity, outliers, and
                        non-numeric data. Applications of value prediction include credit card fraud detection and
                        target mailing list identification.
         34.2.2 Database Segmentation
                        The aim of database segmentation is to partition a database into an unknown number of
                        segments, or clusters, of similar records, that is, records that share a number of properties and
                        so are considered to be homogeneous. (Segments have high internal homogeneity and high
                        external heterogeneity.) This approach uses unsupervised learning to discover homogeneous
                                                       www.it-ebooks.info


                                                                           34.2 Data Mining Techniques      |    1237
sub-populations in a database to improve the accuracy of the profiles. Database segmenta-         Figure 34.3
tion is less precise than other operations and is therefore less sensitive to redundant and       An example
irrelevant features. Sensitivity can be reduced by ignoring a subset of the attributes that       of database
describe each instance or by assigning a weighting factor to each variable. Applications of       segmentation using
                                                                                                  a scatterplot.
database segmentation include customer profiling, direct marketing, and cross-selling. An
example of database segmentation using a scatterplot is shown in Figure 34.3.
   In this example, the database consists of 200 observations: 100 genuine and 100 forged
banknotes. The data is six-dimensional with each dimension corresponding to a particular
measurement of the size of the banknotes. Using database segmentation, we identify the
clusters that correspond to legal tender and forgeries. Note that there are two clusters of
forgeries, which is attributed to at least two gangs of forgers working on falsifying the
banknotes (Girolami et al., 1997).
   Database segmentation is associated with demographic or neural clustering techniques,
which are distinguished by the allowable data inputs, the methods used to calculate the dis-
tance between records, and the presentation of the resulting segments for analysis.
Link Analysis                                                                                     34.2.3
Link analysis aims to establish links, called associations, between the individual records,
or sets of records, in a database. There are three specializations of link analysis: associations
discovery, sequential pattern discovery, and similar time sequence discovery.
   Associations discovery finds items that imply the presence of other items in the same
event. These affinities between items are represented by association rules. For example,
‘when a customer rents property for more than two years and is more than 25 years old, in
                                             www.it-ebooks.info


1238        |    Chapter 34 z Data Mining
                         40% of cases, the customer will buy a property. This association happens in 35% of all
                         customers who rent properties.’
                            Sequential pattern discovery finds patterns between events such that the presence of one
                         set of items is followed by another set of items in a database of events over a period of
                         time. For example, this approach can be used to understand long-term customer buying
                         behavior.
                            Similar time sequence discovery is used, for example, in the discovery of links between
                         two sets of data that are time-dependent, and is based on the degree of similarity between
                         the patterns that both time series demonstrate. For example, within three months of
                         buying property, new home owners will purchase goods such as cookers, freezers, and
                         washing machines.
                            Applications of link analysis include product affinity analysis, direct marketing, and
                         stock price movement.
         34.2.4 Deviation Detection
                         Deviation detection is a relatively new technique in terms of commercially available data
                         mining tools. However, deviation detection is often a source of true discovery because it
Figure 34.4              identifies outliers, which express deviation from some previously known expectation and
An example of            norm. This operation can be performed using statistics and visualization techniques or as
visualization of         a by-product of data mining. For example, linear regression facilitates the identification of
the data shown           outliers in data while modern visualization techniques display summaries and graphical
in Figure 34.3.          representations that make deviations easy to detect. In Figure 34.4, we demonstrate the
                                                      www.it-ebooks.info


                                                                       34.3 The Data Mining Process | 1239
visualization technique on the data shown in Figure 34.3. Applications of deviation detec-
tion include fraud detection in the use of credit cards and insurance claims, quality control,
and defects tracing.
The Data Mining Process                                                                         34.3
Recognizing that a systematic approach is essential to successful data mining, many
vendor and consulting organizations have specified a process model designed to guide the
user (especially someone new to building predictive models) through a sequence of
steps that will lead to good results. In 1996 a consortium of vendors and users consisting
of NCR Systems Engineering Copenhagen (Denmark), Daimler-Benz AG (Germany),
SPSS/Integral Solutions Ltd (England) and OHRA Verzekeringen en Bank Groep BV
(The Netherlands) developed a specification called the Cross Industry Standard Process for
Data Mining (CRISP-DM).
   CRISP-DM specifies a data mining process model that is not specific to any particular
industry or tool. CRISP-DM has evolved from the knowledge discovery processes used
widely in industry and in direct response to user requirements. The major aims of CRISP-
DM are to make large data mining projects run more efficiently as well as to make them
cheaper, more reliable, and more manageable. The current version of CRISP-DM is
Version 1.0 and in this section we briefly describe this model (CRISP-DM, 1996).
The CRISP-DM Model                                                                             34.3.1
The CRISP-DM methodology is a hierarchical process model. At the top level, the process
is divided into six different generic phases, ranging from business understanding to
deployment of project results. The next level elaborates each of these phases as compris-
ing several generic tasks. At this level, the description is generic enough to cover all the
DM scenarios.
   The third level specializes these tasks for specific situations. For instance, the generic
task might be cleaning data, and the specialized task could be cleaning of numeric or
categorical values. The fourth level is the process instance, that is, a record of actions,
decisions, and result of an actual execution of a DM project.
   The model also discusses relationships between different DM tasks. It gives an idealised
sequence of actions during a DM project. However, it does not attempt to give all possible
routes through the tasks. The different phases of the model are shown in Table 34.3.
   The aim of each phase of the CRISP-DM model and the tasks associated with each are
described briefly below.
Business understanding
This phase focuses on understanding the project objectives and requirements from the
business point of view. This phase converts the business problem to a data mining prob-
lem definition and prepares the preliminary plan for the project. The various tasks involved
are as follows: determine business objectives; assess situation; determine data mining
goal; and produce a project plan.
                                            www.it-ebooks.info


1240 | Chapter 34 z Data Mining
                                        Table 34.3    Phases of the CRISP-DM Model.
                                         Phase
                                         Business understanding
                                         Data understanding
                                         Data preparation
                                         Modeling
                                         Evaluation
                                         Deployment
               Data understanding
               This phase includes tasks for initial collection of the data and is concerned with establish-
               ing the main characterisitics of the data. Characteristics include the data structures, data
               quality, and identifying any interesting subsets of the data. The tasks involved in this phase
               are as follows: collect initial data; describe data; explore data; and verify data quality.
               Data preparation
               This phase involves all the activities for constructing the final data set on which modeling
               tools can be applied directly. The different tasks in this phase are as follows: select data;
               clean data; construct data; integrate data; and format data.
               Modeling
               This phase is the actual data mining operation and involves selecting modeling techniques,
               selecting modeling parameters, and assessing the model created. The tasks in this phase are
               as follows: select modeling technique; generate test design; build model; and assess model.
               Evaluation
               This phase validates the model from the data analysis point of view. The model and
               the steps in modeling are verified within the context of achieving the business goals. The
               tasks involved in this phase are as follows: evaluate results; review process; and determine
               next steps.
               Deployment
               The knowledge gained in the form of the model needs to be organized and presented in
               a form that is understood by the business users. The deployment phase can be as simple
               as generating a report or as complex as implementing repeatable DM processing across
               the enterprise. The business user normally executes the deployment phase. The steps
               involved are as follows: plan deployment; plan monitoring and maintenance; produce final
               report; and review report.
                  For a full description of the CRISP-DM model, the interested reader is referred to
               CRISP-DM (1996).
                                             www.it-ebooks.info


                                                                               34.4 Data Mining Tools | 1241
Data Mining Tools                                                                                34.4
There are a growing number of commercial data mining tools on the marketplace. The
important features of data mining tools include data preparation, selection of data mining
operations (algorithms), product scalability and performance, and facilities for under-
standing results.
Data preparation
Data preparation is the most time-consuming aspect of data mining. Whatever a tool can
provide to facilitate this process will greatly speed up model development. Some of the
functions that a tool may provide to support data preparation include: data cleansing, such
as handling missing data; data describing, such as the distribution of values; data trans-
forming, such as performing calculations on existing columns; and data sampling for the
creation of training and validation data sets.
Selection of data mining operations (algorithms)
It is important to understand the characteristics of the operations (algorithms) used by a data
mining tool to ensure that they meet the user’s requirements. In particular, it is important
to establish how the algorithms treat the data types of the response and predictor variables,
how fast they train, and how fast they work on new data. (A predictor variable is the col-
umn in a database that can be used to build a predictor model, to predict values in another
column.)
    Another important feature of an algorithm is its sensitivity to noise. (Noise is the
difference between a model and its predictions. Sometimes data is referred to as being
noisy when it contains errors such as many missing or incorrect values or when there are
extraneous columns.) It is important to establish how sensitive a given algorithm is to
missing data, and how robust are the patterns it discovers in the face of extraneous and
incorrect data.
Product scalability and performance
Scalability and performance are important considerations when seeking a tool that is
capable of dealing with increasing amounts of data in terms of numbers of rows and
columns possibly with sophisticated validation controls. The need to provide scalability
while maintaining satisfactory performance may require investigations into whether a tool
is capable of supporting parallel processing using technologies such as SMP or MPP. We
discuss parallel processing using SMP and MPP technology in Section 23.1.
Facilities for understanding results
A good data mining tool should help the user understand the results by providing measures
such as those describing accuracy and significance in useful formats (for example, con-
fusion matrices) by allowing the user to perform sensitivity analysis on the result, and by
presenting the result in alternative ways (using, for example, visualization techniques).
    A confusion matrix shows the counts of the actual versus predicted class values. It
shows not only how well the model predicts, but also presents the details needed to see
exactly where things may have gone wrong.
                                            www.it-ebooks.info


1242  | Chapter 34 z Data Mining
                   Sensitivity analysis determines the sensitivity of a predictive model to small fluctuations
                in predictor value. Through this technique end-users can gauge the effects of noise and
                environmental change on the accuracy of the model.
                   Visualization graphically displays data to facilitate better understanding of its meaning.
                Graphical capabilities range from simple scatterplots to complex multi-dimensional
                representations.
     34.5       Data Mining and Data Warehousing
                One of the major challenges for organizations seeking to exploit data mining is identify-
                ing suitable data to mine. Data mining requires a single, separate, clean, integrated, and
                self-consistent source of data. A data warehouse is well equipped for providing data for
                mining for the following reasons:
                n  Data quality and consistency are prerequisites for mining to ensure the accuracy of the
                   predictive models. Data warehouses are populated with clean, consistent data.
                n  It is advantageous to mine data from multiple sources to discover as many interrela-
                   tionships as possible. Data warehouses contain data from a number of sources.
                n  Selecting the relevant subsets of records and fields for data mining requires the query
                   capabilities of the data warehouse.
                n  The results of a data mining study are useful if there is some way to further investigate the
                   uncovered patterns. Data warehouses provide the capability to go back to the data source.
                Given the complementary nature of data mining and data warehousing, many vendors are
                investigating ways of integrating data mining and data warehouse technologies.
     34.6       Oracle Data Mining (ODM)
                In large data warehouse environments, many different types of analysis can occur. In addi-
                tion to SQL queries, we may also apply more advanced analytical operations on the data.
                Two major types of analysis are Online Analytical Processing (OLAP) and data mining.
                Rather than having a separate OLAP or data mining engine, Oracle has integrated OLAP
                and data mining capabilities directly into the database server. Oracle OLAP and Oracle
                Data Mining (ODM) are options to the Oracle9i Database. In Section 33.6.7 we presented
                an introduction to Oracle’s support for OLAP, while in this section we provide an intro-
                duction to Oracle’s support for data mining (Oracle Corporation, 2004(j)).
     34.6.1 Data Mining Capabilities
                Oracle enables data mining inside the database for performance and scalability. Some of
                the capabilities include:
                n  an API that provides programmatic control and application integration;
                n  analytical capabilities with OLAP and statistical functions in the database;
                                             www.it-ebooks.info


                                                                    34.6 Oracle Data Mining (ODM) | 1243
n  multiple algorithms: Naïve Bayes, Decision Trees, Clustering, and Association Rules;
n  real-time and batch scoring modes;
n  multiple prediction types;
n  association insights.
Enabling Data Mining Applications                                                            34.6.2
Oracle9i Data Mining provides a Java API to exploit the data mining functionality that
is embedded within the Oracle9i database. By delivering complete programmatic control
of the database in data mining, Oracle Data Mining (ODM) delivers powerful, scalable
modeling and real-time scoring. This enables e-Businesses to incorporate predictions and
classifications in all processes and decision points throughout the business cycle.
   ODM is designed to meet the challenges of vast amounts of data, delivering accurate
insights completely integrated into e-Business applications. This integrated intelligence
enables the automation and decision speed that e-Businesses require in order to compete
in today’s business environment.
Predictions and Insights                                                                     34.6.3
Oracle Data Mining uses data mining algorithms to sift through the large volumes of
data generated by e-Businesses to produce, evaluate, and deploy predictive models. It
also enriches mission-critical applications in customer relationship management (CRM),
manufacturing control, inventory management, customer service and support, Web portals,
wireless devices and other fields with context-specific recommendations and predictive
monitoring of critical processes. ODM delivers real-time answers to questions such as:
n  Which N items is person A most likely to buy or like?
n  What is the likelihood that this product will be returned for repair?
Oracle Data Mining Environment                                                               34.6.4
The Oracle Data Mining environment supports all the phases of data mining within
the database. For each phase the ODM environment results in significant improvements in
such areas as performance, automation, and integration.
Data preparation
Data preparation can create new tables or views of existing data. Both options perform
faster than moving data to an external data mining utility and offer the programmer the
option of snapshots or real-time updates.
   Oracle Data Mining provides utilities for complex, data mining-specific tasks.
Binning improves model build time and model performance, so ODM provides a utility
                                            www.it-ebooks.info


1244 | Chapter 34 z Data Mining
               for user-defined binning. ODM accepts data in either single record format or in trans-
               actional format and performs mining on transactional formats. Single record format is
               most common in applications, so ODM provides a utility for transforming single record
               format.
                  Associated analysis for preparatory data exploration and model evaluation is extended
               by Oracle’s statistical functions and OLAP capabilities. Because these also operate within
               the database, they can all be incorporated into a seamless application that shares database
               objects. This allows for more functional and faster applications.
               Model building
               Oracle Data Mining provides four algorithms: Naïve Bayes, Decision Tree, Clustering,
               and Association Rules. These algorithms address a broad spectrum of business problems,
               ranging from predicting the future likelihood of a customer purchasing a given product, to
               understanding which products are likely be purchased together in a single trip to the
               grocery store. All model building takes place inside the database. Once again, the data
               does not need to move outside the database in order to build the model, and therefore the
               entire data mining process is accelerated.
               Model evaluation
               Models are stored in the database and directly accessible for evaluation, reporting, and fur-
               ther analysis by a wide variety of tools and application functions. ODM provides APIs for
               calculating traditional confusion matrices and lift charts. It stores the models, the under-
               lying data, and these analysis results together in the database to allow further analysis,
               reporting, and application-specific model management.
               Scoring
               Oracle Data Mining provides both batch and real-time scoring. In batch mode, ODM takes
               a table as input. It scores every record, and returns a scored table as a result. In real-time
               mode, parameters for a single record are passed in and the scores are returned in a Java
               object.
                  In both modes, ODM can deliver a variety of scores. It can return a rating or prob-
               ability of a specific outcome. Alternatively it can return a predicted outcome and the
               probability of that outcome occurring. Some examples follow.
               n  How likely is this event to end in outcome A?
               n  Which outcome is most likely to result from this event?
               n  What is the probability of each possible outcome for this event?
                                            www.it-ebooks.info


                                                                                   Chapter Summary        |  1245
Chapter Summary
n Data mining is the process of extracting valid, previously unknown, comprehensible, and actionable
  information from large databases and using it to make crucial business decisions.
n There are four main operations associated with data mining techniques: predictive modeling, database
  segmentation, link analysis, and deviation detection.
n Techniques are specific implementations of the operations (algorithms) that are used to carry out the data
  mining operations. Each operation has its own strengths and weaknesses.
n Predictive modeling can be used to analyze an existing database to determine some essential characteristics
  (model) about the data set. The model is developed using a supervised learning approach, which has two
  phases: training and testing. Applications of predictive modeling include customer retention management,
  credit approval, cross-selling, and direct marketing. There are two associated techniques: classification and
  value prediction.
n Database segmentation partitions a database into an unknown number of segments, or clusters, of similar
  records. This approach uses unsupervised learning to discover homogeneous sub-populations in a database to
  improve the accuracy of the profiles.
n Link analysis aims to establish links, called associations, between the individual records, or sets of records,
  in a database. There are three specializations of link analysis: associations discovery, sequential pattern
  discovery, and similar time sequence discovery. Associations discovery finds items that imply the presence
  of other items in the same event. Sequential pattern discovery finds patterns between events such that the
  presence of one set of items is followed by another set of items in a database of events over a period of time.
  Similar time sequence discovery is used, for example, in the discovery of links between two sets of data
  that are time-dependent, and is based on the degree of similarity between the patterns that both time series
  demonstrate.
n Deviation detection is often a source of true discovery because it identifies outliers, which express deviation
  from some previously known expectation and norm. This operation can be performed using statistics and
  visualization techniques or as a by-product of data mining.
n The Cross Industry Standard Process for Data Mining (CRISP-DM) specification describes a data mining
  process model that is not specific to any particular industry or tool.
n The important characteristics of data mining tools include: data preparation facilities; selection of data min-
  ing operations (algorithms); scalability and performance; and facilities for understanding results.
n A data warehouse is well equipped for providing data for mining as a warehouse not only holds data of high
  quality and consistency, and from multiple sources, but is also capable of providing subsets (views) of the data
  for analysis and lower level details of the source data, when required.
                                          www.it-ebooks.info


1246      |  Chapter 34 z Data Mining
  Review Questions
  34.1 Discuss what data mining represents.                  34.4 Describe the main aims and phases of the
  34.2 Provide examples of data mining applications.               CRISP-DM model.
  34.3 Describe how the following data mining                34.5 Provide examples of important features of data
        operations are applied and provide typical                 mining tools.
        examples for each:                                   34.6 Discuss the relationship between data
        (a) predictive modeling,                                   warehousing and data mining.
        (b) database segmentation,                           34.7 Discuss how Oracle supports data mining.
        (c) link analysis,
        (d) deviation detection.
Exercises
34.8 Consider how a company such as DreamHome could benefit from data mining. Discuss, using examples, the
      data mining operations which could be most usefully applied within DreamHome.
34.9 Investigate whether your organization (such as your university/college or workplace) has invested in data min-
      ing technologies and, if yes, whether the data mining tool(s) forms part of a larger investment in business intel-
      ligence technologies. If possible, establish the reasons for the interest in data mining, how the tools are being
      applied, and whether the promise of data mining has been realized.
                                                    www.it-ebooks.info
